{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "from dgl import DGLGraph\n",
    "import numpy as np\n",
    "import scipy.sparse as spp\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm\n",
    "from dgl.data import DGLDataset\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from ApolloScape_Dataset import ApolloScape_DGLDataset\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from models.My_GAT import My_GAT\n",
    "from models.GCN import GCN\n",
    "from models.Gated_GCN import GatedGCN\n",
    "from models.gnn_rnn import Model_GNN_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ApolloScape_DGLDataset(train_val='train') #3447\n",
    "val_dataset = ApolloScape_DGLDataset(train_val='val')  #919\n",
    "test_dataset = ApolloScape_DGLDataset(train_val='test')  #230\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1060, 12, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collate function to prepare graphs\n",
    "def collate_batch(samples):\n",
    "    graphs, masks = map(list, zip(*samples))  # samples is a list of pairs (graph, mask) mask es VxTx1\n",
    "    masks = np.vstack(masks)\n",
    "    masks = torch.tensor(masks)#+torch.zeros(2)\n",
    "    #masks = masks.view(masks.shape[0],-1)\n",
    "    #masks= masks.view(masks.shape[0]*masks.shape[1],masks.shape[2],masks.shape[3])#.squeeze(0) para TAMAÑO FIJO\n",
    "    sizes_n = [graph.number_of_nodes() for graph in graphs] # graph sizes\n",
    "    snorm_n = [torch.FloatTensor(size, 1).fill_(1 / size) for size in sizes_n]\n",
    "    snorm_n = torch.cat(snorm_n).sqrt()  # graph size normalization \n",
    "    sizes_e = [graph.number_of_edges() for graph in graphs] # nb of edges\n",
    "    snorm_e = [torch.FloatTensor(size, 1).fill_(1 / size) for size in sizes_e]\n",
    "    snorm_e = torch.cat(snorm_e).sqrt()  # graph size normalization\n",
    "    batched_graph = dgl.batch(graphs)  # batch graphs\n",
    "    return batched_graph, masks, snorm_n, snorm_e\n",
    "\n",
    "\n",
    "dev='cuda'\n",
    "batch_size=128\n",
    "train_dataloader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, collate_fn=collate_batch)\n",
    "val_dataloader=DataLoader(val_dataset, batch_size=batch_size, shuffle=False,  num_workers=8, collate_fn=collate_batch)\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=8, collate_fn=collate_batch)  \n",
    "batched_graph, masks, snorm_n, snorm_e=next(iter(test_dataloader))\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.sum(dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "batched_graph, masks, snorm_n, snorm_e, last_vis_obj=next(iter(test_dataloader))\n",
    "print(masks.shape)\n",
    "ori_data=batched_graph.ndata['x'].float()\n",
    "data = ori_data.detach().clone().cpu().numpy()\n",
    "print(data[:,:,0])\n",
    "\n",
    "#new_mask = (data[:, 1:,:2]!=0) * (data[:, :-1, :2]!=0) \n",
    "#data[:, 1:,:2] = (data[:, 1:,:2] - data[:, :-1, :2]).float() * new_mask.float()\n",
    "#data[:, 0, :2] = 0\n",
    "'''\n",
    "rescale_xy = torch.ones((1,1,2)).to(dev)\n",
    "rescale_xy[:,:,0] = torch.max(abs(data[:,:,0]))\n",
    "rescale_xy[:,:,1] = torch.max(abs(data[:,:,1]))\n",
    "data[:,:,:2]=data[:,:,:2]/rescale_xy\n",
    "print(data[0])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open('../DBU_Graph/data/apollo_train_data.pkl', 'rb') as reader:\n",
    "    [feat,adj, mean]=pickle.load(reader)\n",
    "    \n",
    "feat=np.transpose(feat, (0,3,2,1))\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#pruebas def preprocess_data de main.py (GRIP) \n",
    "feature_id = [3, 4, 9, 10]  #x,y,heading,[visible_mask]\n",
    "vel_data = feat[:,feature_id]  # N,C,T,V\n",
    "vel_mask = (vel_data[:, :2, 1:]!=0) * (vel_data[:, :2, :-1]!=0) #False-> frames en los que no tenemos VELOCIDAD del obj\n",
    "vel_data[:, :2, 1:] = (vel_data[:, :2, 1:] - vel_data[:, :2, :-1]).astype(float) * vel_mask.astype(float)\n",
    "vel_data[:, :2, 0] = 0\n",
    "print(vel_data[0,:,6:,0])\n",
    "#print(new_mask[0,:,:,0])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5010, 70, 12, 11])\n"
     ]
    }
   ],
   "source": [
    "features=torch.from_numpy(feat[:,:70,:,:]).type(torch.float32)\n",
    "#print(features.shape)\n",
    "\n",
    "last_vis_obj_i=[]   #contains number of visible obj in each sequence of the training\n",
    "\n",
    "for idx in range(len(adj)): \n",
    "    for i in range(len(adj[idx])): \n",
    "        if adj[idx][i,i] == 0:\n",
    "            last_vis_obj_i.append(i)\n",
    "            break   \n",
    "            \n",
    "last_vis_obj_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "    if (this.ratio !== 1) {\n",
       "        fig.send_message('set_dpi_ratio', { dpi_ratio: this.ratio });\n",
       "    }\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    var resizeObserver = new ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    resizeObserver.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.one(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"639.8333333333334\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "graph=dgl.DGLGraph(spp.coo_matrix(adj[1][:last_vis_obj[1],:last_vis_obj[1]]))\n",
    "nx.draw(graph.to_networkx(), with_labels=True)\n",
    "plt.subplot(1,2,2)\n",
    "graph=dgl.DGLGraph(spp.coo_matrix(adj[2][:last_vis_obj[2],:last_vis_obj[2]]))\n",
    "nx.draw(graph.to_networkx(), with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear grafo (Ejemplo secuencia 0)\n",
    "Para el entrenamiento iterar sobre todas las secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "now_history_frame=6\n",
    "object_type = features[:,:,:,2].int()  # torch Tensor NxVxT\n",
    "#vis_obj_type=np.zeros((features.shape[0],features.shape[1])) #NxV\n",
    "mask_car=np.zeros((features.shape[0],features.shape[1],now_history_frame)) #NxVx6\n",
    "for i in range(len(features)):\n",
    "    #vis_obj_type[i,:] =object_type[i,:,5] #Append Vx1 #tipos de obj visibles de la primera seq    \n",
    "    mask_car_t=np.array([1  if (j==2 or j==1) else 0 for j in object_type[i,:,5]])  #1 si obj 1/2 en frame 5 (size V)\n",
    "    mask_car[i,:]=np.array(mask_car_t).reshape(mask_car.shape[1],1)+np.zeros(6) #Vx6 mask para los 6 output frames que indican si el obj es visible y car\n",
    "\n",
    "#COMPROBADO OK\n",
    "feature_id = [3, 4, 9]  #x,y,heading,[visible_mask]\n",
    "#120 agentes (13 visibles -> feat[11]=1) y 12 frames (si no hay info en alguno de os 12 frames: fila nula)\n",
    "node_features = features[:,:,:now_history_frame,feature_id]  #obj type,x,y 6 primeros frames\n",
    "node_labels=features[:,:,now_history_frame:,3:5] #x,y 6 ultimos frames\n",
    "node_features[:,:,:,-1] *= mask_car   #Pongo 0 en feat 11 [mask] a todos los obj visibles no-car\n",
    "print(node_labels.shape)\n",
    "node_labels[:,:,:,-1] *= mask_car\n",
    "output_mask= features[:,:,6:,-1]*mask_car #mascara obj (car) visibles en 6º frame (5010,120,6)\n",
    "output_mask.unsqueeze_(-1).type(torch.uint8)    #N,V,T,1                  \n",
    "\n",
    "print(node_features.shape, node_labels.shape,output_mask.shape)\n",
    "zero_indeces_list = [i for i in range(len(output_mask)) if np.all(np.array(output_mask.squeeze(-1))==0, axis=(1,2))[i] == True ]\n",
    "zero_maskcar_list = [i for i in range(len(mask_car)) if np.all(np.array(mask_car)==0, axis=(1,2))[i] == True ]\n",
    "#len(zero_maskcar_list) #374\n",
    "\n",
    "total_num = len(features)\n",
    "id_list = list(set(list(range(total_num))) - set(zero_indeces_list))\n",
    "total_valid_num = len(id_list) #4596\n",
    "ind=np.random.permutation(id_list)\n",
    "train_id_list, val_id_list = ind[:round(total_valid_num*0.8)], ind[round(total_valid_num*0.8):]\n",
    "\n",
    "print(len(train_id_list)) #3677\n",
    "xy_dist=[spatial.distance.cdist(node_features[i][:,5,:], node_features[i][:,5,:]) for i in range(len(features))]  #5010x120x120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "#Perform message passing and then apply fc Layer (self-loops! - same W for neighbors and itself)\n",
    "# Traditional GCN:\n",
    "#fn.copy_src(src='h', out='m')\n",
    "#gcn_reduce = fn.sum(msg='m', out='h')\n",
    "# multiply source node features with edge weights and aggregate them in destination nodes\n",
    "gcn_msg=fn.u_mul_e('h', 'w', 'm') #elemnt-wise (broadcast)\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, dropout):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear_self = nn.Linear(in_feats, out_feats, bias=False)\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "        \n",
    "    def reduce_func(self, nodes):\n",
    "        h = torch.sum(nodes.mailbox['m'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, g, feature,e_w, snorm_n, snorm_e):\n",
    "        # Creating a local scope so that all the stored ndata and edata\n",
    "        # (such as the `'h'` ndata below) are automatically popped out\n",
    "        # when the scope exits.\n",
    "        with g.local_scope():\n",
    "            \n",
    "            if self.dropout:\n",
    "                feature = self.dropout(feature)\n",
    "            \n",
    "            g.ndata['h_s']=self.linear_self(feature)\n",
    "            \n",
    "            #normalization\n",
    "            degs = g.out_degrees().float().clamp(min=1)\n",
    "            norm=torch.pow(degs,-0.5)\n",
    "            shp = norm.shape + (1,)*(feature.dim() -1)\n",
    "            norm = torch.reshape(norm,shp)\n",
    "            feature = feature*norm\n",
    "            \n",
    "            #aggregate\n",
    "            g.edata['w'] = e_w\n",
    "            g.ndata['h'] = feature\n",
    "            g.update_all(gcn_msg, self.reduce_func)\n",
    "            \n",
    "            #mult W and normalization\n",
    "            h = self.linear(g.ndata['h'])\n",
    "            degs = g.in_degrees().float().clamp(min=1)\n",
    "            norm = torch.pow(degs, -0.5)\n",
    "            shp = norm.shape + (1,) * (feature.dim() - 1)\n",
    "            norm = torch.reshape(norm, shp)\n",
    "            h = h * norm\n",
    "            \n",
    "            h = g.ndata['h_s'] + h #Vx6xout_feats\n",
    "            \n",
    "            #h = h * (torch.ones_like(h)*snorm_n)  # normalize activation w.r.t. graph node size\n",
    "            #e_w =  e_w * (torch.ones_like(e_w)*snorm_e)  # normalize activation w.r.t. graph edge size\n",
    "            e_w =  e_w\n",
    "            \n",
    "            return h, e_w\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding_h = nn.Linear(in_feats, hid_feats)\n",
    "        self.conv1 = GCNLayer(in_feats=hid_feats, out_feats=hid_feats)\n",
    "        self.conv2 = GCNLayer(in_feats=hid_feats, out_feats=hid_feats)\n",
    "        self.fc= nn.Linear(hid_feats,out_feats)\n",
    "        self.dropout = dropout\n",
    "        if dropout:\n",
    "            self.linear_dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.linear_dropout = 0.\n",
    "    def forward(self, graph, inputs,e_w,snorm_n, snorm_e):\n",
    "        # input embedding\n",
    "        h = self.embedding_h(inputs)\n",
    "        h,_ = self.conv1(graph, h,e_w,snorm_n, snorm_e,self.dropout) #Vx6x4 -> Vx6x32  \n",
    "        h = F.relu(h)\n",
    "        h,_ = self.conv2(graph,h,e_w,snorm_n, snorm_e,self.dropout)  #Vx6x2 \n",
    "        h = F.relu(h)\n",
    "        h = self.linear_dropout(h)\n",
    "        y = self.fc(h)\n",
    "        return y\n",
    "\n",
    "'''\n",
    "from dgl.nn import GatedGraphConv, GraphConv, GATConv,SAGEConv\n",
    "conv = GraphConv(4,2, weight=True, bias=True)\n",
    "#sageconv = SAGEConv(4,2,aggregator_type='lstm')\n",
    "#gated_conv = GatedGraphConv(4, 2, 2, 3)\n",
    "#gatconv=GATConv(4,2,num_heads=4)\n",
    "graph = dgl.add_self_loop(graph)   #Añado selfloops pq no puede haber zero in-degree nodes\n",
    "res = conv(graph, node_features[0])\n",
    "print(res.shape)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My_GAT(\n",
      "  (embedding_h): Linear(in_features=24, out_features=128, bias=True)\n",
      "  (gat_1): My_GATLayer(\n",
      "    (linear_self): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (linear_func): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (attention_func): Linear(in_features=256, out_features=1, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (gat_2): My_GATLayer(\n",
      "    (linear_self): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (linear_func): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (attention_func): Linear(in_features=256, out_features=1, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (linear1): Linear(in_features=128, out_features=12, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class My_GATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, feat_drop=0., attn_drop=0.):\n",
    "        super(My_GATLayer, self).__init__()\n",
    "        self.linear_self = nn.Linear(in_feats, out_feats, bias=False)\n",
    "        self.linear_func = nn.Linear(in_feats, out_feats, bias=False)\n",
    "        self.attention_func = nn.Linear(2 * out_feats, 1, bias=False)\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        #self.bn_node = nn.BatchNorm1d(out_feats)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.linear_self.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.linear_func.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attention_func.weight, gain=gain)\n",
    "    \n",
    "    def edge_attention(self, edges):\n",
    "        concat_z = torch.cat([edges.src['z'], edges.dst['z']], dim=-1) #(n_edg,6*64)||(n_edg,6*64) -> (n_edg,2*6*64) \n",
    "        src_e = self.attention_func(concat_z)  #(n_edg, 1) att logit\n",
    "        src_e = F.leaky_relu(src_e)\n",
    "        return {'e': src_e}\n",
    "    \n",
    "    def message_func(self, edges):\n",
    "        return {'z': edges.src['z'], 'e':edges.data['e']}\n",
    "        \n",
    "    def reduce_func(self, nodes):\n",
    "        h_s = nodes.data['h_s']\n",
    "        \n",
    "        #ATTN DROPOUT\n",
    "        a = self.attn_drop(   F.softmax(nodes.mailbox['e'], dim=1)  )  #attention score between nodes i and j\n",
    "        \n",
    "        h = h_s + torch.sum(a * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "                               \n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            \n",
    "            #feat dropout\n",
    "            h=self.feat_drop(h)\n",
    "            \n",
    "            h_in = h\n",
    "            g.ndata['h']  = h \n",
    "            g.ndata['h_s'] = self.linear_self(h) \n",
    "            g.ndata['z'] = self.linear_func(h) #(18) -> (18) \n",
    "            g.apply_edges(self.edge_attention)\n",
    "            g.update_all(self.message_func, self.reduce_func)\n",
    "            h = g.ndata['h'] # result of graph convolution\n",
    "            #h = h * snorm_n # normalize activation w.r.t. graph node size\n",
    "            #h = self.bn_node(h) # batch normalization \n",
    "            \n",
    "            h = torch.relu(h) # non-linear activation\n",
    "            h = h_in + h # residual connection\n",
    "            \n",
    "            return h #graph.ndata.pop('h')\n",
    "\n",
    "\n",
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(My_GATLayer(in_feats, out_feats))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        head_outs = [attn_head(g, h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1), for intermediate layers\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average, for final layer\n",
    "            return torch.mean(torch.stack(head_outs))\n",
    "\n",
    "class MLP_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, input_dim/2) \n",
    "        self.layer2 = nn.Linear(input_dim/2, input_dim/4) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.layer1(y)\n",
    "        y = torch.relu(y)\n",
    "        y = self.layer2(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "class My_GAT(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout, feat_drop=0., attn_drop=0., heads=4):\n",
    "        super().__init__()\n",
    "        self.embedding_h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gat_1 = My_GATLayer(hidden_dim, hidden_dim, feat_drop, attn_drop)\n",
    "        self.gat_2 = My_GATLayer(hidden_dim, hidden_dim, feat_drop, attn_drop)\n",
    "        #self.gat_1 = MultiHeadGATLayer(hidden_dim, hidden_dim, heads)\n",
    "        #self.gat_2 = MultiHeadGATLayer(hidden_dim*heads, hidden_dim*heads, 1)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_dim, output_dim) #hidden*heads para multihead\n",
    "        #self.linear2 = nn.Linear( int(hidden_dim/2),  output_dim)\n",
    "        \n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "        \n",
    "    def forward(self, g, h,e_w,snorm_n,snorm_e):\n",
    "        \n",
    "        # input embedding\n",
    "        h = self.embedding_h(h)  #input (70, 6,4) - (70, 6,32) checked\n",
    "        # gat layers\n",
    "        h = self.gat_1(g, h)\n",
    "        h = self.gat_2(g, h)\n",
    "        \n",
    "        h = self.dropout(h)\n",
    "        y = self.linear1(h) \n",
    "        #y = self.linear2(torch.relu(y))\n",
    "        return y\n",
    "    \n",
    "print( My_GAT(input_dim=24, hidden_dim=128, output_dim=12, dropout=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated GCN \n",
    "$$\n",
    "\\def \\vx {\\boldsymbol{\\color{Plum}{x}}}\n",
    "\\def \\vh {\\boldsymbol{\\color{YellowGreen}{h}}}\n",
    "\\def \\ve {\\boldsymbol{\\color{purple}{e}}}\n",
    "\\def \\aqua#1{\\color{Aquamarine}{#1}}\n",
    "\\def \\red#1{\\color{OrangeRed}{#1}}\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "\\vh &= \\vx + \\Big( A \\vx +  \\sum_{\\aqua{v}_j \\to \\red{v}} \\eta(\\ve_{j}) \\odot B \\vx_j \\Big)^+\\\\\n",
    "\\eta(\\ve_{j}) &= \\sigma(\\ve_{j})\\Big(\\sum_{\\aqua{v}_k \\to \\red{v}} \\sigma(\\ve_{k})\\Big)^{-1} \\\\\n",
    "\\ve_{j} &= C \\ve_{j}^{\\vx} + D \\vx_j + E\\vx\\\\\n",
    "\\ve_{j}^{\\vh} &= \\ve_j^{\\vx} + \\Big( \\ve_{j}  \\Big)^+\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GatedGCN_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.A = nn.Linear(input_dim, output_dim)\n",
    "        self.B = nn.Linear(input_dim, output_dim)\n",
    "        self.C = nn.Linear(input_dim, output_dim)\n",
    "        self.D = nn.Linear(input_dim, output_dim)\n",
    "        self.E = nn.Linear(input_dim, output_dim)\n",
    "        self.bn_node_h = nn.BatchNorm1d(output_dim)\n",
    "        self.bn_node_e = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        Bh_j = edges.src['Bh']\n",
    "        # e_ij = Ce_ij + Dhi + Ehj   N*B,256\n",
    "        e_ij = edges.data['Ce'] + edges.src['Dh'] + edges.dst['Eh']\n",
    "        edges.data['e'] = e_ij\n",
    "        return {'Bh_j' : Bh_j, 'e_ij' : e_ij}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        Ah_i = nodes.data['Ah']\n",
    "        Bh_j = nodes.mailbox['Bh_j']\n",
    "        e = nodes.mailbox['e_ij']\n",
    "        # sigma_ij = sigmoid(e_ij)\n",
    "        \n",
    "        torch.clamp(e.sigmoid_(), min=1e-4, max=1-1e-4) \n",
    "        sigma_ij = torch.sigmoid(e)\n",
    "        # hi = Ahi + sum_j eta_ij * Bhj   \n",
    "        h = Ah_i + torch.sum(sigma_ij * Bh_j, dim=1) / torch.sum(sigma_ij, dim=1)  #shape n_nodes*256\n",
    "        return {'h' : h}\n",
    "    \n",
    "    def forward(self, g, h, e, snorm_n, snorm_e):\n",
    "        \n",
    "        h_in = h # residual connection\n",
    "        e_in = e # residual connection\n",
    "        \n",
    "        \n",
    "        g.ndata['h']  = h \n",
    "        g.ndata['Ah'] = self.A(h) \n",
    "        g.ndata['Bh'] = self.B(h) \n",
    "        g.ndata['Dh'] = self.D(h)\n",
    "        g.ndata['Eh'] = self.E(h) \n",
    "        g.edata['e']  = e \n",
    "        g.edata['Ce'] = self.C(e)\n",
    "        \n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        \n",
    "        h = g.ndata['h'] # result of graph convolution\n",
    "        e = g.edata['e'] # result of graph convolution\n",
    "\n",
    "        \n",
    "        h = h * snorm_n # normalize activation w.r.t. graph node size\n",
    "        e = e * snorm_e # normalize activation w.r.t. graph edge size\n",
    "        \n",
    "        h = self.bn_node_h(h) # batch normalization  \n",
    "        e = self.bn_node_e(e) # batch normalization  \n",
    "        \n",
    "        h = torch.relu(h) # non-linear activation\n",
    "        e = torch.relu(e) # non-linear activation\n",
    "        \n",
    "        h = h_in + h # residual connection\n",
    "        e = e_in + e # residual connection\n",
    "        \n",
    "        return h, e\n",
    "    \n",
    "class GatedGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.embedding_e = nn.Linear(1, hidden_dim)\n",
    "        self.GatedGCN1 = GatedGCN_layer(hidden_dim, hidden_dim)\n",
    "        self.GatedGCN2 = GatedGCN_layer(hidden_dim, hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, g, h, e, snorm_n, snorm_e):\n",
    "        # input embedding\n",
    "        h = self.embedding_h(h)\n",
    "        e = self.embedding_e(e)\n",
    "        # graph convnet layers\n",
    "        h, e = self.GatedGCN1(g, h, e, snorm_n, snorm_e)\n",
    "        h, e = self.GatedGCN2(g, h, e, snorm_n, snorm_e)\n",
    "        # MLP \n",
    "        y = self.linear1(h)\n",
    "        \n",
    "        return y\n",
    "\n",
    "print( GatedGCN(input_dim=18, hidden_dim=64, output_dim=12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightning Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: voe9e855\n",
      "Sweep URL: https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g4tnh6lx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: gcn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msandracl72\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">misty-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sandracl72/dbu_graph\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/g4tnh6lx\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/g4tnh6lx</a><br/>\n",
       "                Run data is saved locally in <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201119_155330-g4tnh6lx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcn\n",
      "############### TRAIN ####################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g4tnh6lx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9070<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3be7cad15e42b897445a73d0eaaed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201119_155330-g4tnh6lx/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201119_155330-g4tnh6lx/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">misty-sweep-1</strong>: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/g4tnh6lx\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/g4tnh6lx</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:g4tnh6lx). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Resuming run <strong style=\"color:#cdcd00\">misty-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sandracl72/dbu_graph\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/g4tnh6lx\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/g4tnh6lx</a><br/>\n",
       "                Run data is saved locally in <code>./logs/wandb/run-20201119_155331-g4tnh6lx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GCN  | 18 K  \n",
      "/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69df7064449b48ac85d329c101904096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1501983bc9af42f6b7c6bc81b4e3696d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9098<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e659a37a5d84621b4c20b785688fa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>./logs/wandb/run-20201119_155331-g4tnh6lx/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>./logs/wandb/run-20201119_155331-g4tnh6lx/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val/Loss</td><td>174.94234</td></tr><tr><td>_step</td><td>0</td></tr><tr><td>_runtime</td><td>4</td></tr><tr><td>_timestamp</td><td>1605797617</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val/Loss</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr><tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">misty-sweep-1</strong>: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/g4tnh6lx\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/g4tnh6lx</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run g4tnh6lx errored: DGLError('Caught DGLError in DataLoader worker process 0.\\nOriginal Traceback (most recent call last):\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\\n    data = fetcher.fetch(index)\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\\n    return self.collate_fn(data)\\n  File \"<ipython-input-2-937fb2f09b21>\", line 15, in collate_batch\\n    return batched_graph.to(dev), masks.to(dev), snorm_n.to(dev), snorm_e.to(dev)\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/heterograph.py\", line 5005, in to\\n    ret._graph = self._graph.copy_to(utils.to_dgl_context(device))\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/heterograph_index.py\", line 234, in copy_to\\n    return _CAPI_DGLHeteroCopyTo(self, ctx.device_type, ctx.device_id)\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 287, in dgl._ffi._cy3.core.FunctionBase.__call__\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 222, in dgl._ffi._cy3.core.FuncCall\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 211, in dgl._ffi._cy3.core.FuncCall3\\n  File \"dgl/_ffi/_cython/./base.pxi\", line 155, in dgl._ffi._cy3.core.CALL\\ndgl._ffi.base.DGLError: [15:53:37] /opt/dgl/src/runtime/cuda/cuda_device_api.cc:93: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: initialization error\\nStack trace:\\n  [bt] (0) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x4f) [0x7fec2ff768af]\\n  [bt] (1) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::CUDADeviceAPI::AllocDataSpace(DLContext, unsigned long, unsigned long, DLDataType)+0x283) [0x7fec30768e53]\\n  [bt] (2) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::Empty(std::vector<long, std::allocator<long> >, DLDataType, DLContext)+0x177) [0x7fec30627d47]\\n  [bt] (3) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::CopyTo(DLContext const&) const+0xc0) [0x7fec3065cc10]\\n  [bt] (4) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::UnitGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DLContext const&)+0x2eb) [0x7fec3073c00b]\\n  [bt] (5) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::HeteroGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DLContext const&)+0xf5) [0x7fec3066df25]\\n  [bt] (6) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(+0xecdfbb) [0x7fec3067afbb]\\n  [bt] (7) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(DGLFuncCall+0x48) [0x7fec3060bde8]\\n  [bt] (8) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/_ffi/_cy3/core.cpython-38-x86_64-linux-gnu.so(+0x16f99) [0x7fec2e001f99]\\n\\n\\n')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run g4tnh6lx errored: DGLError('Caught DGLError in DataLoader worker process 0.\\nOriginal Traceback (most recent call last):\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\\n    data = fetcher.fetch(index)\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\\n    return self.collate_fn(data)\\n  File \"<ipython-input-2-937fb2f09b21>\", line 15, in collate_batch\\n    return batched_graph.to(dev), masks.to(dev), snorm_n.to(dev), snorm_e.to(dev)\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/heterograph.py\", line 5005, in to\\n    ret._graph = self._graph.copy_to(utils.to_dgl_context(device))\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/heterograph_index.py\", line 234, in copy_to\\n    return _CAPI_DGLHeteroCopyTo(self, ctx.device_type, ctx.device_id)\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 287, in dgl._ffi._cy3.core.FunctionBase.__call__\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 222, in dgl._ffi._cy3.core.FuncCall\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 211, in dgl._ffi._cy3.core.FuncCall3\\n  File \"dgl/_ffi/_cython/./base.pxi\", line 155, in dgl._ffi._cy3.core.CALL\\ndgl._ffi.base.DGLError: [15:53:37] /opt/dgl/src/runtime/cuda/cuda_device_api.cc:93: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: initialization error\\nStack trace:\\n  [bt] (0) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x4f) [0x7fec2ff768af]\\n  [bt] (1) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::CUDADeviceAPI::AllocDataSpace(DLContext, unsigned long, unsigned long, DLDataType)+0x283) [0x7fec30768e53]\\n  [bt] (2) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::Empty(std::vector<long, std::allocator<long> >, DLDataType, DLContext)+0x177) [0x7fec30627d47]\\n  [bt] (3) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::CopyTo(DLContext const&) const+0xc0) [0x7fec3065cc10]\\n  [bt] (4) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::UnitGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DLContext const&)+0x2eb) [0x7fec3073c00b]\\n  [bt] (5) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::HeteroGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DLContext const&)+0xf5) [0x7fec3066df25]\\n  [bt] (6) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(+0xecdfbb) [0x7fec3067afbb]\\n  [bt] (7) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(DGLFuncCall+0x48) [0x7fec3060bde8]\\n  [bt] (8) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/_ffi/_cy3/core.cpython-38-x86_64-linux-gnu.so(+0x16f99) [0x7fec2e001f99]\\n\\n\\n')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3pobvosn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: gat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">vivid-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sandracl72/dbu_graph\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/3pobvosn\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/3pobvosn</a><br/>\n",
       "                Run data is saved locally in <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201119_155341-3pobvosn</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gat\n",
      "############### TRAIN ####################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3pobvosn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9170<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aae1a241adf48c8af3cbf533beb3dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201119_155341-3pobvosn/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201119_155341-3pobvosn/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">vivid-sweep-2</strong>: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/3pobvosn\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/3pobvosn</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3pobvosn). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Resuming run <strong style=\"color:#cdcd00\">vivid-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sandracl72/dbu_graph\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/3pobvosn\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/3pobvosn</a><br/>\n",
       "                Run data is saved locally in <code>./logs/wandb/run-20201119_155342-3pobvosn</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | My_GAT | 18 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005565b2018c47499b304b54cc344153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21567f9e77cc4f6cbacf542c56c9212b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9198<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511bf40728cf418b932ecb3d474bba80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>./logs/wandb/run-20201119_155342-3pobvosn/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>./logs/wandb/run-20201119_155342-3pobvosn/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val/Loss</td><td>189.18094</td></tr><tr><td>_step</td><td>0</td></tr><tr><td>_runtime</td><td>4</td></tr><tr><td>_timestamp</td><td>1605797628</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val/Loss</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr><tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">vivid-sweep-2</strong>: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/3pobvosn\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/3pobvosn</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 3pobvosn errored: DGLError('Caught DGLError in DataLoader worker process 0.\\nOriginal Traceback (most recent call last):\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\\n    data = fetcher.fetch(index)\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\\n    return self.collate_fn(data)\\n  File \"<ipython-input-2-937fb2f09b21>\", line 15, in collate_batch\\n    return batched_graph.to(dev), masks.to(dev), snorm_n.to(dev), snorm_e.to(dev)\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/heterograph.py\", line 5005, in to\\n    ret._graph = self._graph.copy_to(utils.to_dgl_context(device))\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/heterograph_index.py\", line 234, in copy_to\\n    return _CAPI_DGLHeteroCopyTo(self, ctx.device_type, ctx.device_id)\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 287, in dgl._ffi._cy3.core.FunctionBase.__call__\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 222, in dgl._ffi._cy3.core.FuncCall\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 211, in dgl._ffi._cy3.core.FuncCall3\\n  File \"dgl/_ffi/_cython/./base.pxi\", line 155, in dgl._ffi._cy3.core.CALL\\ndgl._ffi.base.DGLError: [15:53:48] /opt/dgl/src/runtime/cuda/cuda_device_api.cc:93: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: initialization error\\nStack trace:\\n  [bt] (0) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x4f) [0x7fec2ff768af]\\n  [bt] (1) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::CUDADeviceAPI::AllocDataSpace(DLContext, unsigned long, unsigned long, DLDataType)+0x283) [0x7fec30768e53]\\n  [bt] (2) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::Empty(std::vector<long, std::allocator<long> >, DLDataType, DLContext)+0x177) [0x7fec30627d47]\\n  [bt] (3) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::CopyTo(DLContext const&) const+0xc0) [0x7fec3065cc10]\\n  [bt] (4) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::UnitGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DLContext const&)+0x2eb) [0x7fec3073c00b]\\n  [bt] (5) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::HeteroGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DLContext const&)+0xf5) [0x7fec3066df25]\\n  [bt] (6) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(+0xecdfbb) [0x7fec3067afbb]\\n  [bt] (7) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(DGLFuncCall+0x48) [0x7fec3060bde8]\\n  [bt] (8) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/_ffi/_cy3/core.cpython-38-x86_64-linux-gnu.so(+0x16f99) [0x7fec2e001f99]\\n\\n\\n')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 3pobvosn errored: DGLError('Caught DGLError in DataLoader worker process 0.\\nOriginal Traceback (most recent call last):\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\\n    data = fetcher.fetch(index)\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\\n    return self.collate_fn(data)\\n  File \"<ipython-input-2-937fb2f09b21>\", line 15, in collate_batch\\n    return batched_graph.to(dev), masks.to(dev), snorm_n.to(dev), snorm_e.to(dev)\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/heterograph.py\", line 5005, in to\\n    ret._graph = self._graph.copy_to(utils.to_dgl_context(device))\\n  File \"/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/heterograph_index.py\", line 234, in copy_to\\n    return _CAPI_DGLHeteroCopyTo(self, ctx.device_type, ctx.device_id)\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 287, in dgl._ffi._cy3.core.FunctionBase.__call__\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 222, in dgl._ffi._cy3.core.FuncCall\\n  File \"dgl/_ffi/_cython/./function.pxi\", line 211, in dgl._ffi._cy3.core.FuncCall3\\n  File \"dgl/_ffi/_cython/./base.pxi\", line 155, in dgl._ffi._cy3.core.CALL\\ndgl._ffi.base.DGLError: [15:53:48] /opt/dgl/src/runtime/cuda/cuda_device_api.cc:93: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: initialization error\\nStack trace:\\n  [bt] (0) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x4f) [0x7fec2ff768af]\\n  [bt] (1) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::CUDADeviceAPI::AllocDataSpace(DLContext, unsigned long, unsigned long, DLDataType)+0x283) [0x7fec30768e53]\\n  [bt] (2) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::Empty(std::vector<long, std::allocator<long> >, DLDataType, DLContext)+0x177) [0x7fec30627d47]\\n  [bt] (3) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::CopyTo(DLContext const&) const+0xc0) [0x7fec3065cc10]\\n  [bt] (4) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::UnitGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DLContext const&)+0x2eb) [0x7fec3073c00b]\\n  [bt] (5) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(dgl::HeteroGraph::CopyTo(std::shared_ptr<dgl::BaseHeteroGraph>, DLContext const&)+0xf5) [0x7fec3066df25]\\n  [bt] (6) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(+0xecdfbb) [0x7fec3067afbb]\\n  [bt] (7) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/libdgl.so(DGLFuncCall+0x48) [0x7fec3060bde8]\\n  [bt] (8) /home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/_ffi/_cy3/core.cpython-38-x86_64-linux-gnu.so(+0x16f99) [0x7fec2e001f99]\\n\\n\\n')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qv9h5upm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dims: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: gated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">astral-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sandracl72/dbu_graph\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/sweeps/voe9e855</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/qv9h5upm\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/qv9h5upm</a><br/>\n",
       "                Run data is saved locally in <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201119_155352-qv9h5upm</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gated\n",
      "############### TRAIN ####################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qv9h5upm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9264<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72701e22922244678b046d7eac07b952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "class LitGNN(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module = GCN, lr: float = 1e-3, batch_size: int = 64, model_type: str = 'gcn'):\n",
    "        super().__init__()\n",
    "        self.model= model\n",
    "        self.lr = lr\n",
    "        self.model_type = model_type\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch=0\n",
    "        \n",
    "        self.overall_loss_train=[]\n",
    "        \n",
    "        self.overall_num_list=[]\n",
    "        self.overall_x2y2_list=[]\n",
    "        \n",
    "        self.test_overall_num_list=[]\n",
    "        self.test_overall_x2y2_list=[]\n",
    "        \n",
    "        wandb.watch(self.model, log=\"all\")\n",
    "        \n",
    "    \n",
    "    def forward(self, graph, feats,e_w,snorm_n,snorm_e):\n",
    "        pred = self.model(graph, feats,e_w,snorm_n,snorm_e)   #inference\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return opt\n",
    "    \n",
    "    def compute_RMSE_batch(self,pred, gt, mask): \n",
    "        pred=pred.view(pred.shape[0],mask.shape[1],-1)\n",
    "        pred = pred*mask #B*V,T,C  (B n grafos en el batch)\n",
    "        gt = gt*mask  # outputmask BV,T,C\n",
    "        x2y2_error=torch.sum(torch.abs(pred-gt)**2,dim=-1) # x^2+y^2 BV,T\n",
    "        overall_sum_time = x2y2_error.sum(dim=-2)  #T - suma de los errores (x^2+y^2) de los V agentes\n",
    "        overall_num = mask.sum(dim=-1).type(torch.int)  #torch.Tensor[(T)] - num de agentes (Y CON DATOS) en cada frame\n",
    "        return overall_sum_time, overall_num, x2y2_error\n",
    "\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        batched_graph, output_masks,snorm_n, snorm_e = train_batch\n",
    "        feats = batched_graph.ndata['x'].float()\n",
    "        #reshape to have shape (B*V,T*C) [c1,c2,...,c6]\n",
    "        feats = feats.view(feats.shape[0],-1)\n",
    "        e_w = batched_graph.edata['w'].float()\n",
    "        if self.model_type == 'gated':\n",
    "            e_w= e_w.view(e_w.shape[0],1)\n",
    "        labels= batched_graph.ndata['gt'].float()\n",
    "        pred = self.model(batched_graph, feats,e_w,snorm_n,snorm_e)\n",
    "        overall_sum_time, overall_num, _ = self.compute_RMSE_batch(pred, labels, output_masks)  #(B,6)\n",
    "        total_loss=torch.sum(overall_sum_time)/torch.sum(overall_num.sum(dim=-2))\n",
    "        self.overall_loss_train.extend([total_loss.data.item()])\n",
    "        #self.log('train_loss',total_loss, prog_bar=True, logger=True, on_epoch=True)\n",
    "        return total_loss\n",
    "    \n",
    "    def training_epoch_end(self, total_loss):\n",
    "        self.epoch += 1\n",
    "        print('|{}| Train_loss: {}'.format(datetime.now(), np.sum(self.overall_loss_train)/len(self.overall_loss_train)))\n",
    "        #self.log(\"Train/Loss\", np.sum(self.overall_loss_train)/len(self.overall_loss_train) )\n",
    "        self.logger.log_metrics({\"Train/loss\": np.sum(self.overall_loss_train)/len(self.overall_loss_train)}, step=self.epoch)\n",
    "        #wandb.log({\"Train/loss\": np.sum(self.overall_loss_train)/len(self.overall_loss_train)}, step=self.epoch)#, step=epoch)      \n",
    "        self.overall_loss_train=[]\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        batched_graph, output_masks,snorm_n, snorm_e = val_batch\n",
    "        feats = batched_graph.ndata['x'].float()\n",
    "        #reshape to have shape (B*V,T*C) [c1,c2,...,c6]\n",
    "        feats = feats.view(feats.shape[0],-1)\n",
    "        e_w = batched_graph.edata['w'].float()\n",
    "        if self.model_type == 'gated':\n",
    "            e_w= e_w.view(e_w.shape[0],1)\n",
    "        labels= batched_graph.ndata['gt'].float()\n",
    "        pred = self.model(batched_graph, feats,e_w,snorm_n,snorm_e)\n",
    "        _, overall_num, x2y2_error = self.compute_RMSE_batch(pred, labels, output_masks)\n",
    "        self.overall_num_list.extend(overall_num.detach().cpu().numpy())\n",
    "        self.overall_x2y2_list.extend((x2y2_error**0.5).detach().cpu().numpy())  \n",
    "        \n",
    "    def validation_epoch_end(self, val_results):\n",
    "        overall_sum_time=np.sum(self.overall_x2y2_list,axis=0)  #BV,T->T RMSE medio en cada T\n",
    "        overall_num_time =np.sum(self.overall_num_list, axis=0)\n",
    "        overall_loss_time=(overall_sum_time / overall_num_time) #media del error de cada agente en cada frame\n",
    "        \n",
    "        self.overall_num_list=[]\n",
    "        self.overall_x2y2_list=[]\n",
    "        #self.log('val/Loss',np.sum(overall_loss_time), on_step=)\n",
    "        self.logger.log_metrics({'val/Loss':np.sum(overall_loss_time)}, step= self.epoch)\n",
    "        #wandb.log({\"Val/Loss\": np.sum(overall_loss_time)}, step= self.epoch)\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        batched_graph, output_masks,snorm_n, snorm_e = test_batch\n",
    "        feats = batched_graph.ndata['x'].float()\n",
    "        #reshape to have shape (B*V,T*C) [c1,c2,...,c6]\n",
    "        feats = feats.view(feats.shape[0],-1)\n",
    "        e_w = batched_graph.edata['w'].float()\n",
    "        if self.model_type == 'gated':\n",
    "            e_w= e_w.view(e_w.shape[0],1)\n",
    "        labels= batched_graph.ndata['gt'].float()\n",
    "        pred = self.model(batched_graph, feats,e_w,snorm_n,snorm_e)\n",
    "        _, overall_num, x2y2_error = self.compute_RMSE_batch(pred, labels, output_masks)\n",
    "        self.test_overall_num_list.extend(overall_num.detach().cpu().numpy())\n",
    "        self.test_overall_x2y2_list.extend((x2y2_error**0.5).detach().cpu().numpy())\n",
    "                  \n",
    "    def test_epoch_end(self,test_results):\n",
    "        overall_sum_time=np.sum(self.test_overall_x2y2_list,axis=0)  #BV,T->T RMSE medio en cada T\n",
    "        overall_num_time =np.sum(self.test_overall_num_list, axis=0)\n",
    "        overall_loss_time=(overall_sum_time / overall_num_time)\n",
    "        self.log('test/loss', np.sum(overall_loss_time))\n",
    "        #self.log('test/loss_per_sec',' '.join(['{:.3f}'.format(x) for x in list(overall_loss_time)]))\n",
    "        self.log('test/loss_per_sec', overall_loss_time)    \n",
    "\n",
    "        #wandb.log({'test/loss': np.sum(overall_loss_time)})    \n",
    "\n",
    "\n",
    "def sweep_train():\n",
    "\n",
    "    hyperparameters_default = dict (\n",
    "        batch_size = batch_size,\n",
    "        learning_rate = learning_rate,\n",
    "        hidden_dims = hidden_dims,\n",
    "        model_type = model_type,\n",
    "        epochs = total_epoch\n",
    "    )\n",
    "\n",
    "    wandb.init(config= hyperparameters_default) \n",
    "    config = wandb.config\n",
    "    wandb_logger = pl_loggers.WandbLogger(save_dir='./logs/')  #name=\n",
    "    train_dataloader=DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=12, collate_fn=collate_batch)\n",
    "\n",
    "    print(config.model_type)\n",
    "    if config.model_type == 'gat':\n",
    "        model = My_GAT(input_dim=18, hidden_dim=config.hidden_dims, output_dim=12)\n",
    "    elif config.model_type == 'gcn':\n",
    "        model = GCN(in_feats=18, hid_feats=config.hidden_dims, out_feats=12)\n",
    "    elif config.model_type == 'gated':\n",
    "        model = GatedGCN(input_dim=18, hidden_dim=config.hidden_dims, output_dim=12)\n",
    "\n",
    "    LitGNN_sys = LitGNN(model=model, lr=config.learning_rate, model_type= config.model_type)\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=20,logger=wandb_logger, check_val_every_n_epoch=3, precision=16,  profiler=True)  #precision=16, callbacks=[early_stop_callback],limit_train_batches=0.5, progress_bar_refresh_rate=20, \n",
    "    \n",
    "    print(\"############### TRAIN ####################\")\n",
    "    trainer.fit(LitGNN_sys, train_dataloader, val_dataloader)   \n",
    "\n",
    "    print(\"############### TEST ####################\")\n",
    "    trainer.test(test_dataloaders=test_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "total_epoch = 20\n",
    "learning_rate =1e-3\n",
    "hidden_dims=256\n",
    "model_type= 'gat'\n",
    "\n",
    "sweep_config = {\n",
    "\"name\": \"Sweep pynb\",\n",
    "\"method\": \"grid\",\n",
    "\"metric\": {\n",
    "  'name': 'val/Loss',\n",
    "  'goal': 'minimize'   \n",
    "        },\n",
    "\"parameters\": {\n",
    "        \"batch_size\": {\n",
    "            \"values\": [32,64,128]\n",
    "        },\n",
    "        \"hidden_dims\": {\n",
    "            \"values\": [64, 128, 256]\n",
    "        },\n",
    "        \"model_type\": {\n",
    "            \"values\": ['gcn', 'gat', 'gated']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"dbu_graph\")\n",
    "\n",
    "wandb.agent(sweep_id, sweep_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightningmodel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        \\ntotal_epoch = 20\\nlr =1e-3\\nhidden_dims=64\\nmodel_type= \\'rnn\\'\\n\\n\\nwandb.init(project=\"dbu_graph\", config={\"epochs\": total_epoch, \"batch_size\": batch_size, \"learning_rate\": lr,\"model_architecture\": model, \"model_type\":model_type \"hidden_dims\": hidden_dims})\\nconfig = wandb.config\\nwandb_logger = pl_loggers.WandbLogger(save_dir=\\'./logs/\\')  #name=\\n\\n\\nif model_type == \\'gat\\':\\n    model = My_GAT(input_dim=18, hidden_dim=hidden_dims, output_dim=12)\\nelif model_type == \\'gcn\\':\\n    model = model = GCN(in_feats=18, hid_feats=hidden_dims, out_feats=12)\\nelif model_type == \\'gated\\':\\n    model = GatedGCN(input_dim=18, hidden_dim=hidden_dims, output_dim=12)\\nelif model_type == \\'rnn\\':\\n    model = Model_GNN_RNN()\\n    \\n#init model\\nLitGCN = LitGCN(model=model, model_type=\\'gat\\', lr=lr)\\n    \\n# most basic trainer, uses good defaults (auto-tensorboard, checkpoints, logs, and more)\\n# trainer = pl.Trainer(gpus=8) (if you have GPUs)\\n# using only half the training data and checking validation every quarter of a training epoch\\nearly_stop_callback = EarlyStopping(\\n   monitor=\\'val_loss\\',\\n   min_delta=0.00,\\n   patience=3,\\n   verbose=False,\\n   mode=\\'max\\'\\n)\\ntrainer = pl.Trainer(gpus=1, lr=config.learning_rate, logger=wandb_logger, max_epochs=config.epochs, progress_bar_refresh_rate=20, precision=16, profiler=True)  #val_check_interval=0.25\\n\\n\\ntrain_dataloader=DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8,  collate_fn=collate_batch)\\nval_dataloader=DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True,  collate_fn=collate_batch)\\n\\ntrainer.fit(LitGCN, train_dataloader, val_dataloader)\\ntrainer.test(test_dataloaders=test_dataloader)\\n\\n!tensorboard --logdir ./lightning_logs\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "class LitGNN(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module = GCN, model_type: str = 'gat', lr: float = 1e-3, batch_size: int = 64,  wd: float = 1e-1):\n",
    "        super().__init__()\n",
    "        self.model= model\n",
    "        self.lr = lr\n",
    "        self.model_type = model_type\n",
    "        self.batch_size = batch_size\n",
    "        self.wd = wd\n",
    "        self.overall_loss_time_list=[]\n",
    "    \n",
    "    def forward(self, graph, feats,e_w,snorm_n,snorm_e):\n",
    "        pred = self.model(graph, feats,e_w,snorm_n,snorm_e)   #inference\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=self.wd)\n",
    "        return opt\n",
    "    \n",
    "    def compute_RMSE_batch(self,pred, gt, mask):\n",
    "        pred = pred*mask #B*V,T,C  (B n grafos en el batch)\n",
    "        gt = gt*mask  # outputmask BV,T,C\n",
    "        x2y2_error=torch.sum(torch.abs(pred-gt)**2,dim=-1) # x^2+y^2 BV,T\n",
    "        overall_sum_time = x2y2_error.sum(dim=-2)  #T - suma de los errores (x^2+y^2) de los V agentes\n",
    "        overall_num = mask.sum(dim=-1).type(torch.int)  #torch.Tensor[(BV,T)] - num de agentes (Y CON DATOS) en cada frame\n",
    "        return overall_sum_time, overall_num, x2y2_error\n",
    "\n",
    "    def compute_change_pos(self, feats,gt):\n",
    "        gt_vel = gt.detach().clone()\n",
    "        feats_vel = feats[:,:,:2].detach().clone()\n",
    "        new_mask_feats = (feats_vel[:, 1:,:2]!=0) * (feats_vel[:, :-1, :2]!=0) \n",
    "        new_mask_gt = (gt_vel[:, 1:,:2]!=0) * (gt_vel[:, :-1, :2]!=0) \n",
    "\n",
    "        gt_vel[:, 1:,:2] = (gt_vel[:, 1:,:2] - gt_vel[:, :-1, :2]).float() * new_mask_gt.float()\n",
    "        gt_vel[:, :1, :2] = (gt_vel[:, 0:1,:2] - feats_vel[:, -1:, :2]).float()\n",
    "        feats_vel[:, 1:,:2] = (feats_vel[:, 1:,:2] - feats_vel[:, :-1, :2]).float() * new_mask_feats.float()\n",
    "        feats_vel[:, 0, :2] = 0\n",
    "        \n",
    "        return feats_vel.float(), gt_vel.float()\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        '''needs to return a loss from a single batch'''\n",
    "\n",
    "        batched_graph, output_masks,snorm_n, snorm_e = train_batch\n",
    "        feats = batched_graph.ndata['x'].float()\n",
    "        #reshape to have shape (B*V,T*C) [c1,c2,...,c6]\n",
    "        feats = feats.view(feats.shape[0],-1)\n",
    "        e_w = batched_graph.edata['w'].float()\n",
    "        if self.model_type != 'gcn':\n",
    "            e_w= e_w.view(e_w.shape[0],1)\n",
    "        labels= batched_graph.ndata['gt'].float()\n",
    "        pred = self.model(batched_graph, feats,e_w,snorm_n,snorm_e)\n",
    "        overall_sum_time, overall_num, _ = self.compute_RMSE_batch(pred, labels, output_masks)  #(B,6)\n",
    "        total_loss=torch.sum(overall_sum_time)/torch.sum(overall_num.sum(dim=-2))\n",
    "        ##self.overall_loss_train.extend([total_loss.data.item()])\n",
    "\n",
    "        # Log metrics\n",
    "        self.logger.agg_and_log_metrics({\"Train/loss\": total_loss.data.item()}, step=self.current_epoch)\n",
    "        #self.log(\"Train/loss\",  total_loss.data.item(), on_step=False, on_epoch=True)\n",
    "        return total_loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        batched_graph, output_masks,snorm_n, snorm_e = val_batch\n",
    "        feats = batched_graph.ndata['x'].float()\n",
    "        feats = feats.view(feats.shape[0],-1)\n",
    "        e_w = batched_graph.edata['w'].float()\n",
    "        if self.model_type != 'gcn':\n",
    "            e_w= e_w.view(e_w.shape[0],1)\n",
    "        labels= batched_graph.ndata['gt'].float()\n",
    "        pred = self.model(batched_graph, feats,e_w,snorm_n,snorm_e)\n",
    "        _, overall_num, x2y2_error = self.compute_RMSE_batch(pred, labels, output_masks)\n",
    "        overall_loss_time = np.sum((x2y2_error**0.5).detach().cpu().numpy(), axis=0) / np.sum(overall_num.detach().cpu().numpy(), axis=0)#T\n",
    "        self.logger.agg_and_log_metrics({'val/Loss':np.sum(overall_loss_time)}, step= self.current_epoch)\n",
    "        #self.log('val/Loss', np.sum(overall_loss_time) )\n",
    "        \n",
    "        \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        batched_graph, output_masks,snorm_n, snorm_e = test_batch\n",
    "        feats = batched_graph.ndata['x'].float()\n",
    "        labels= batched_graph.ndata['gt'][:,:,:2].float()\n",
    "        last_loc = feats[:,-1:,:2]\n",
    "        #USE CHANGE IN POS AS INPUT\n",
    "        feats_vel,_ = self.compute_change_pos(feats,labels)\n",
    "        #Input pos + vel\n",
    "        feats = torch.cat([feats[:,:,:], feats_vel], dim=-1)\n",
    "\n",
    "        e_w = batched_graph.edata['w'].float()\n",
    "        if self.model_type != 'gcn':\n",
    "            e_w= e_w.view(e_w.shape[0],1)\n",
    "\n",
    "        pred = self.model(batched_graph, feats,e_w,snorm_n,snorm_e)\n",
    "        pred=pred.view(pred.shape[0],labels.shape[1],-1)\n",
    "        _, overall_num, x2y2_error = self.compute_RMSE_batch(pred, labels, output_masks[:,6:,:])\n",
    "        overall_loss_time = np.sum((x2y2_error**0.5).detach().cpu().numpy(),axis=0) / np.sum(overall_num.detach().cpu().numpy(), axis=0) #T\n",
    "        if np.any(np.isnan(overall_loss_time)):\n",
    "            print( 'error, num: ', np.sum((x2y2_error**0.5).detach().cpu().numpy(),axis=0), np.sum(overall_num.detach().cpu().numpy(), axis=0))\n",
    "            overall_loss_time[np.isnan(overall_loss_time)]=0\n",
    "        print('per sec loss:{}, Sum{}'.format(overall_loss_time, np.sum(overall_loss_time)))\n",
    "        self.overall_loss_time_list.append(overall_loss_time)\n",
    "        #self.logger.experiment.log({ \"test/loss_per_sec\": overall_loss_time })\n",
    "        self.log_dict({'test/loss': np.sum(overall_loss_time), \"test/loss_1\": torch.tensor(overall_loss_time[:2]), \"test/loss_2\": torch.tensor(overall_loss_time[2:4]), \"test/loss_3\": torch.tensor(overall_loss_time[4:]) })\n",
    " \n",
    "    def on_test_epoch_end(self):\n",
    "        overall_loss_time = np.array(self.overall_loss_time_list)\n",
    "        avg = [sum(overall_loss_time[:,i])/overall_loss_time.shape[0] for i in range(len(overall_loss_time[0]))]\n",
    "        var = [sum(abs(overall_loss_time[:,i]-avg[i]))/overall_loss_time.shape[0] for i in range(len(overall_loss_time[0]))]\n",
    "        print('Loss variance: ' , var)\n",
    "        \n",
    "'''        \n",
    "total_epoch = 20\n",
    "lr =1e-3\n",
    "hidden_dims=64\n",
    "model_type= 'rnn'\n",
    "\n",
    "\n",
    "wandb.init(project=\"dbu_graph\", config={\"epochs\": total_epoch, \"batch_size\": batch_size, \"learning_rate\": lr,\"model_architecture\": model, \"model_type\":model_type \"hidden_dims\": hidden_dims})\n",
    "config = wandb.config\n",
    "wandb_logger = pl_loggers.WandbLogger(save_dir='./logs/')  #name=\n",
    "\n",
    "\n",
    "if model_type == 'gat':\n",
    "    model = My_GAT(input_dim=18, hidden_dim=hidden_dims, output_dim=12)\n",
    "elif model_type == 'gcn':\n",
    "    model = model = GCN(in_feats=18, hid_feats=hidden_dims, out_feats=12)\n",
    "elif model_type == 'gated':\n",
    "    model = GatedGCN(input_dim=18, hidden_dim=hidden_dims, output_dim=12)\n",
    "elif model_type == 'rnn':\n",
    "    model = Model_GNN_RNN()\n",
    "    \n",
    "#init model\n",
    "LitGCN = LitGCN(model=model, model_type='gat', lr=lr)\n",
    "    \n",
    "# most basic trainer, uses good defaults (auto-tensorboard, checkpoints, logs, and more)\n",
    "# trainer = pl.Trainer(gpus=8) (if you have GPUs)\n",
    "# using only half the training data and checking validation every quarter of a training epoch\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.00,\n",
    "   patience=3,\n",
    "   verbose=False,\n",
    "   mode='max'\n",
    ")\n",
    "trainer = pl.Trainer(gpus=1, lr=config.learning_rate, logger=wandb_logger, max_epochs=config.epochs, progress_bar_refresh_rate=20, precision=16, profiler=True)  #val_check_interval=0.25\n",
    "\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8,  collate_fn=collate_batch)\n",
    "val_dataloader=DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True,  collate_fn=collate_batch)\n",
    "\n",
    "trainer.fit(LitGCN, train_dataloader, val_dataloader)\n",
    "trainer.test(test_dataloaders=test_dataloader)\n",
    "\n",
    "!tensorboard --logdir ./lightning_logs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5, 1.5, 2.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5, 0.5, 1.5]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]  #2,3\n",
    "a.append([1,1,1])\n",
    "a.append([2,2,4])\n",
    "a=np.array(a)\n",
    "avg =[sum(a[:,i])/a.shape[0] for i in range(len(a[0]))]\n",
    "print(avg)\n",
    "var = [sum(abs((a[:,i]-avg[i])/a.shape[0])) for i in range(len(a[0]))]\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train (wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msandracl72\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">confused-water-773</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sandracl72/dbu_graph\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/gcombo4z\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/gcombo4z</a><br/>\n",
       "                Run data is saved locally in <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201125_150935-gcombo4z</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:56<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-18228cdf40cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mbatch_e\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m#model = GatedGCN(input_dim=18, hidden_dim=256, output_dim=12).to(dev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mbatch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_graphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_snorm_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_snorm_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;31m#print(batch_pred.shape, masks.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PROGRAMAS/DBU_Graph/models/My_GAT.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, h, e_w, snorm_n, snorm_e)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# gat layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#RELU DENTRO DE LA GAT_LAYER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PROGRAMAS/DBU_Graph/models/My_GAT.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, h)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mhead_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattn_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattn_head\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# concat on the output feature dimension (dim=1), for intermediate layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PROGRAMAS/DBU_Graph/models/My_GAT.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mhead_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattn_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattn_head\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# concat on the output feature dimension (dim=1), for intermediate layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PROGRAMAS/DBU_Graph/models/My_GAT.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, h)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h_s'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(18) -> (18)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# result of graph convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/heterograph.py\u001b[0m in \u001b[0;36mapply_edges\u001b[0;34m(self, func, edges, etype, inplace)\u001b[0m\n\u001b[1;32m   4119\u001b[0m             \u001b[0medata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_gsddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4121\u001b[0;31m             \u001b[0medata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_edge_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_e_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/dgl/core.py\u001b[0m in \u001b[0;36minvoke_edge_udf\u001b[0;34m(graph, eid, etype, func, orig_eid)\u001b[0m\n\u001b[1;32m     80\u001b[0m     ebatch = EdgeBatch(graph, eid if orig_eid is None else orig_eid,\n\u001b[1;32m     81\u001b[0m                        etype, srcdata, edata, dstdata)\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minvoke_udf_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsgdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_nid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PROGRAMAS/DBU_Graph/models/My_GAT.py\u001b[0m in \u001b[0;36medge_attention\u001b[0;34m(self, edges)\u001b[0m\n\u001b[1;32m     42\u001b[0m            \u001b[0mconcat_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0msrc_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_z\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#(n_edg, 1) att logit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0msrc_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'e'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msrc_e\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "#train_iter=iter(train_dataloader)\n",
    "import wandb\n",
    "wandb.init(project=\"dbu_graph\")\n",
    "\n",
    "work_dir = './models_checkpoints'\n",
    "model_type = 'gat'\n",
    "hidden_dims=256\n",
    "batch_train=64\n",
    "batch_val=64\n",
    "base_lr=1e-3\n",
    "total_epoch=20\n",
    "\n",
    "def my_save_model(model):\n",
    "    path = '{}/{}_bt{}bv{}_hid{}_lr{}_ep{:03}.pt'.format(work_dir, model_type, batch_train,batch_val, hidden_dims, base_lr, total_epoch)\n",
    "    if os.path.exists(path):\n",
    "        path= '.' + path.split('.')[1] + '_' + str(datetime.now().minute)+ '.pt'\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print('Successfully saved to {}'.format(path))\n",
    "    \n",
    "    \n",
    "\n",
    "def compute_RMSE_batch(pred, gt, mask): \n",
    "    #output mask vale 0 si no visible o no-car o visible pero no hay datos en ese frame  (B*V,T,1), cada fila un nodo de un grafo perteneciente al batch\n",
    "    pred=pred.view(pred.shape[0],mask.shape[1],-1)\n",
    "    #gt=gt.view(pred.shape[0],6,-1)\n",
    "    \n",
    "    pred = pred*mask #B*V,T,C  (B n grafos en el batch)\n",
    "    gt = gt*mask  # outputmask BV,T,C\n",
    "    \n",
    "    x2y2_error=torch.sum(torch.abs(pred-gt)**2,dim=-1) # x^2+y^2 BV,T\n",
    "    overall_sum_time = x2y2_error.sum(dim=-2)  #T - suma de los errores (x^2+y^2) de los V agentes\n",
    "    overall_num = mask.sum(dim=-1).type(torch.int)  #torch.Tensor[(T)] - num de agentes (Y CON DATOS) en cada frame\n",
    "    return overall_sum_time, overall_num, x2y2_error\n",
    "\n",
    "\n",
    "def val(model, val_dataloader,val_loss_sum):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        overall_num_list=[] \n",
    "        overall_x2y2_list=[]\n",
    "        for batched_graph, output_masks,snorm_n, snorm_e in tqdm(val_dataloader):\n",
    "            feats = batched_graph.ndata['x'].float().to(dev)\n",
    "            #reshape to have shape (B*V,T*C) [c1,c2,...,c6]\n",
    "            feats = feats.view(feats.shape[0],-1)\n",
    "            e_w = batched_graph.edata['w'].float().to(dev)\n",
    "            \n",
    "            if model == 'gated':\n",
    "                e_w= e_w.view(e_w.shape[0],1)\n",
    "            \n",
    "            labels= batched_graph.ndata['gt'].float().to(dev)\n",
    "            #labels = labels.view(labels.shape[0], -1)\n",
    "            pred = model(batched_graph.to(dev), feats,e_w,snorm_n,snorm_e)\n",
    "            _, overall_num, x2y2_error = compute_RMSE_batch(pred, labels, output_masks.to(dev))\n",
    "            #print(x2y2_error.shape)  #BV,T\n",
    "            overall_num_list.extend(overall_num.detach().cpu().numpy())\n",
    "            #print(overall_num.shape)  #BV,T\n",
    "            overall_x2y2_list.extend((x2y2_error**0.5).detach().cpu().numpy())  #RMSE para cada nodo en cada T\n",
    "            \n",
    "    overall_sum_time=np.sum(overall_x2y2_list,axis=0)  #BV,T->T RMSE medio en cada T\n",
    "    overall_num_time =np.sum(overall_num_list, axis=0)\n",
    "    overall_loss_time=(overall_sum_time / overall_num_time) #media del error de cada agente en cada frame\n",
    "\n",
    "    print('|{}| Val_loss: {}'.format(datetime.now(), ' '.join(['{:.3f}'.format(x) for x in list(overall_loss_time) + [np.sum(overall_loss_time)]])))\n",
    "    val_loss_sum.append(np.sum(overall_loss_time))\n",
    "    wandb.log({'Val/Loss': val_loss_sum[-1] }, step=epoch)\n",
    "    \n",
    "\n",
    "dev='cuda:0'\n",
    "if model_type == 'gat':\n",
    "    model = My_GAT(input_dim=24, hidden_dim=hidden_dims, output_dim=12, heads=4, dropout=0.25, att_ew=True).to(dev)\n",
    "elif model_type == 'gcn':\n",
    "    model = model = GCN(in_feats=24, hid_feats=hidden_dims, out_feats=12, dropout=0.25).to(dev)\n",
    "elif model_type == 'gated':\n",
    "    model = GatedGCN(input_dim=24, hidden_dim=hidden_dims, output_dim=12).to(dev)\n",
    "\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
    "np.seterr(all='raise')\n",
    "train_loss_sum=[]\n",
    "val_loss_sum=[]\n",
    "val_loss_prev=0\n",
    "patience=0\n",
    "wandb.watch(model, log='all')\n",
    "n_epochs=0\n",
    "    \n",
    "for epoch in range(total_epoch):\n",
    "    \n",
    "    print(\"Epoch: \",epoch)\n",
    "    overall_loss_train=[]\n",
    "    model.train()\n",
    "    n_epochs=epoch+1\n",
    "    \n",
    "    for batch_graphs, masks, batch_snorm_n, batch_snorm_e in tqdm(train_dataloader):\n",
    "        feats = batch_graphs.ndata['x'].float().to(dev)\n",
    "        feats=feats.view(feats.shape[0],-1)  #Nx18\n",
    "        batch_e = batch_graphs.edata['w'].float().to(dev)\n",
    "        #for GATED GCN\n",
    "        if model_type == 'gated' or model_type == 'gat':\n",
    "            batch_e=batch_e.view(batch_e.shape[0],1)\n",
    "        #model = GatedGCN(input_dim=18, hidden_dim=256, output_dim=12).to(dev)\n",
    "        batch_pred = model(batch_graphs.to(dev), feats, batch_e, batch_snorm_n.to(dev), batch_snorm_e.to(dev))\n",
    "        #print(batch_pred.shape, masks.shape)\n",
    "\n",
    "        labels= batch_graphs.ndata['gt'].float().to(dev)\n",
    "        overall_sum_time, overall_num, _ = compute_RMSE_batch(batch_pred, labels, masks.to(dev))  #(B,6)\n",
    "        total_loss=torch.sum(overall_sum_time)/torch.sum(overall_num.sum(dim=-2))\n",
    "        opt.zero_grad() \n",
    "        total_loss.backward()\n",
    "        #print(model.embedding_h.weight.grad) #model.GatedGCN1.A.weight.grad)\n",
    "        opt.step()\n",
    "        overall_loss_train.extend([total_loss.data.item()])\n",
    "        \n",
    "    print('|{}| Train_loss: {}'.format(datetime.now(), np.sum(overall_loss_train)/len(overall_loss_train)))\n",
    "    train_loss_sum.append(np.sum(overall_loss_train)/len(overall_loss_train))\n",
    "    wandb.log({\"Train/loss\": train_loss_sum[-1]}, step=epoch)\n",
    "    \n",
    "    val(model, val_dataloader, val_loss_sum)\n",
    "    \n",
    "    if val_loss_prev < val_loss_sum[-1] and epoch !=0:\n",
    "        patience+=1\n",
    "        val_loss_prev = val_loss_sum[-1]\n",
    "    else:\n",
    "        patience = 0\n",
    "        val_loss_prev = val_loss_sum[-1]\n",
    "        \n",
    "    if patience > 2:\n",
    "        print(\"Early stopping: \")\n",
    "        print(\"Difference: {}\".format(val_loss_prev-val_loss_sum[-1]))\n",
    "        break\n",
    "        \n",
    "        \n",
    "my_save_model(model)\n",
    "\n",
    "#torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'model.pt'))\n",
    "\n",
    "epochs = list(range(epoch+1))\n",
    "plt.ion() #Turn the interactive mode on\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,train_loss_sum)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,val_loss_sum)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Loss')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1qfp1qie) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 215109<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2e17f28410458bb1e60b1dd8a09f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201130_133628-1qfp1qie/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201130_133628-1qfp1qie/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">graceful-haze-1468</strong>: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/1qfp1qie\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/1qfp1qie</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:1qfp1qie). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">scarlet-sound-1470</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sandracl72/dbu_graph\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/172apwl4\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/172apwl4</a><br/>\n",
       "                Run data is saved locally in <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201130_133743-172apwl4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GatedGCN(\n",
      "  (embedding_h): Linear(in_features=30, out_features=511, bias=True)\n",
      "  (embedding_e): Linear(in_features=1, out_features=511, bias=True)\n",
      "  (GatedGCN1): GatedGCN_layer(\n",
      "    (A): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (B): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (C): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (D): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (E): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (bn_node_h): BatchNorm1d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn_node_e): BatchNorm1d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (GatedGCN2): GatedGCN_layer(\n",
      "    (A): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (B): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (C): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (D): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (E): Linear(in_features=511, out_features=511, bias=True)\n",
      "    (bn_node_h): BatchNorm1d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn_node_e): BatchNorm1d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=511, out_features=12, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:172apwl4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 216109<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e4fe49ce714fd48b7643479bbf7e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201130_133743-172apwl4/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/sandra/PROGRAMAS/DBU_Graph/wandb/run-20201130_133743-172apwl4/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">scarlet-sound-1470</strong>: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/172apwl4\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/172apwl4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:172apwl4). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">wandering-salad-1471</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sandracl72/dbu_graph\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sandracl72/dbu_graph/runs/6xrsejbh\" target=\"_blank\">https://wandb.ai/sandracl72/dbu_graph/runs/6xrsejbh</a><br/>\n",
       "                Run data is saved locally in <code>./logs/wandb/run-20201130_133750-6xrsejbh</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario1/miniconda3/envs/pDL/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db578dbbee284a37a975f733562be966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-85ea97de77d9>:96: RuntimeWarning: invalid value encountered in true_divide\n",
      "  overall_loss_time = np.sum((x2y2_error**0.5).detach().cpu().numpy(),axis=0) / np.sum(overall_num.detach().cpu().numpy(), axis=0) #T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, num:  [ 4.03708205  8.38758644 13.2821671   0.          0.          0.        ] [1 1 1 0 0 0]\n",
      "per sec loss:[ 4.03708205  8.38758644 13.2821671   0.          0.          0.        ], Sum25.70683557847434\n",
      "per sec loss:[ 2.47528645  5.40441878  8.59995356 12.13594097 15.11484074 18.55026384], Sum62.28070433510163\n",
      "per sec loss:[2.67835273 2.84643324 2.78001892 2.80555377 3.28240571 3.83164474], Sum18.224409091172696\n",
      "error, num:  [2.00614187 3.73621116 0.         0.         0.         0.        ] [2 2 0 0 0 0]\n",
      "per sec loss:[1.00307093 1.86810558 0.         0.         0.         0.        ], Sum2.871176513300262\n",
      "per sec loss:[0.2839815  0.48321658 0.12516859 0.26904194 0.26177432 0.30569796], Sum1.7288808936363824\n",
      "per sec loss:[ 1.25801565  2.2018007   3.23738621  7.94127165 10.56869478  9.28642699], Sum34.49359598470592\n",
      "per sec loss:[0.86736411 1.50522296 2.23961048 3.17206015 5.92418881 7.20349957], Sum20.911946076302144\n",
      "per sec loss:[2.43224514 3.27404881 2.88143608 4.23605527 5.48304426 3.43618814], Sum21.74301769841173\n",
      "per sec loss:[ 3.70633915  5.24753501  7.24782733  8.80284162 10.62243414 11.95221548], Sum47.579192728525506\n",
      "per sec loss:[ 4.22825983  8.68434205 12.57143522 17.42123842 22.44736786 27.23186745], Sum92.58451082939756\n",
      "per sec loss:[3.10585511 3.25081754 3.59339159 4.22296035 5.11906003 6.17301784], Sum25.465102474401768\n",
      "per sec loss:[0.21975863 0.48285739 0.2311733  0.09407965 0.8064573  0.99096438], Sum2.8252906503380544\n",
      "per sec loss:[1.56996114 2.56761826 4.55523124 4.67568854 6.41984981 6.66517905], Sum26.453528030087178\n",
      "per sec loss:[ 1.39764843  1.97854348  5.87283503  6.99092225  8.29609752 10.14295508], Sum34.67900179804922\n",
      "per sec loss:[0.98568025 1.47913654 1.5318228  2.30951818 2.96029899 3.34338718], Sum12.609843940873933\n",
      "per sec loss:[2.21997227 3.25301649 4.75294075 5.99025972 5.9575374  6.94221206], Sum29.115938685728064\n",
      "per sec loss:[1.15231593 0.95002642 0.49273565 0.22297336 0.06652102 0.42040742], Sum3.3049797955832436\n",
      "per sec loss:[0.67374573 2.40694924 2.80212439 2.87126825 3.76386939 5.84940937], Sum18.367366370378093\n",
      "per sec loss:[0.47302464 0.6697615  0.29522067 0.5195364  0.22593639 0.43327276], Sum2.6167523490863327\n",
      "per sec loss:[ 9.51314105  8.8103543   8.72041716  8.7963621   9.56410868 10.06791658], Sum55.47229986463325\n",
      "per sec loss:[1.90987131 3.43602891 4.71484999 6.38900035 6.45887002 7.52178112], Sum30.430401687388684\n",
      "per sec loss:[ 3.46546419  6.98753293 10.50345049 14.376307   20.10712582  6.0676295 ], Sum61.507509918397496\n",
      "per sec loss:[0.46783343 1.09726798 2.82780734 1.04551293 0.0836381  1.27767886], Sum6.799738642067019\n",
      "per sec loss:[1.13975038 1.26363379 1.51248658 2.17300611 3.00190827 2.97527051], Sum12.066055634245847\n",
      "per sec loss:[3.62104491 2.9769299  3.1510378  2.50247067 2.99101179 4.54533333], Sum19.787828405956844\n",
      "per sec loss:[1.10554806 1.48733131 2.02247569 2.19958756 2.78901766 3.55983742], Sum13.16379769547682\n",
      "per sec loss:[0.67349079 0.77124522 3.18156054 3.42334957 3.98838644 3.85899693], Sum15.897029505475398\n",
      "per sec loss:[2.27280808 4.21663775 4.85404443 7.33831383 7.51366691 6.43385029], Sum32.6293212866791\n",
      "per sec loss:[ 3.75479979  7.60946147 10.901318   14.56273578 18.34152132  0.7117552 ], Sum55.881591558616236\n",
      "per sec loss:[ 4.81796174  7.3800844  11.00255445 13.81401332 15.65507854 18.71748254], Sum71.38717497697847\n",
      "per sec loss:[1.35758502 1.71612636 2.03867473 2.81940722 3.89652338 4.52987509], Sum16.358191797778804\n",
      "per sec loss:[ 0.90332738  1.6160264   2.28355771  4.04406249 13.85315032 13.69013516], Sum36.390259444473116\n",
      "per sec loss:[ 1.50697147  4.12109685  6.52482473  8.97680898 11.87495155 19.80569523], Sum52.81034881711202\n",
      "per sec loss:[3.92127662 2.27259292 2.67972336 3.19414326 3.44344255 4.36380126], Sum19.87497997810705\n",
      "per sec loss:[1.15517662 2.93068818 0.70166919 1.01408982 1.03241912 1.17229449], Sum8.00633742402457\n",
      "error, num:  [ 2.49122104  2.11241126 10.96639948  0.          0.          0.        ] [1 1 1 0 0 0]\n",
      "per sec loss:[ 2.49122104  2.11241126 10.96639948  0.          0.          0.        ], Sum15.570031777471897\n",
      "per sec loss:[1.67074551 3.22516386 5.65517556 5.32416875 6.51458899 7.83613712], Sum30.22597978281423\n",
      "per sec loss:[0.8609963  1.06356216 1.49960612 1.74601853 2.93323279 3.04439088], Sum11.147806764106006\n",
      "per sec loss:[0.4768145  0.8891986  4.11097323 4.85661135 5.6833069  6.51054805], Sum22.527452614522076\n",
      "per sec loss:[0.83810098 1.70738656 3.13188472 4.46525883 5.91834827 8.4937137 ], Sum24.554693059147315\n",
      "per sec loss:[1.45550558 1.72709157 1.09609653 1.3817311  1.30136135 2.28358658], Sum9.24537270232948\n",
      "per sec loss:[ 1.7352652   4.62003332 16.94851393 17.20181578 19.59202192 21.25022445], Sum81.34787459461691\n",
      "error, num:  [12.06710924 21.86123018 20.65854672  7.21736349  5.84444611  0.        ] [13 11  8  3  2  0]\n",
      "per sec loss:[0.92823917 1.98738456 2.58231834 2.40578783 2.92222305 0.        ], Sum10.8259529581434\n",
      "per sec loss:[0.95239441 1.78111325 2.79883712 4.18714044 5.79258279 7.52685893], Sum23.03892693345806\n",
      "per sec loss:[2.01026002 2.70992523 3.69566422 3.95181633 0.70420105 0.90975973], Sum13.981626590823089\n",
      "per sec loss:[1.82310268 2.58218947 3.40674131 3.03286974 3.79245161 4.28587404], Sum18.92322885140943\n",
      "error, num:  [ 3.51187922  7.14016236  7.52212142 17.28392691  0.          0.        ] [1 1 1 1 0 0]\n",
      "per sec loss:[ 3.51187922  7.14016236  7.52212142 17.28392691  0.          0.        ], Sum35.458089911324905\n",
      "error, num:  [ 4.33289415 23.24002808 21.6696958   7.01151012  0.          0.        ] [7 5 4 2 0 0]\n",
      "per sec loss:[0.61898488 4.64800562 5.41742395 3.50575506 0.         0.        ], Sum14.190169503178254\n",
      "per sec loss:[0.78260392 0.85579605 1.20134817 1.53055104 3.61482273 3.57095188], Sum11.55607380019809\n",
      "per sec loss:[0.20981964 0.33609245 0.17283632 0.32681387 0.44652546 0.66907177], Sum2.161159514997066\n",
      "per sec loss:[1.10250442 1.76702583 1.99207796 2.86620753 3.54411071 4.93540221], Sum16.20732864236787\n",
      "per sec loss:[0.10694677 0.33624987 0.15236031 0.25927036 0.36672233 0.53222219], Sum1.7537718412755048\n",
      "per sec loss:[2.95685115 3.72051001 5.50217049 3.78351412 3.42575111 5.76216916], Sum25.150966050594477\n",
      "per sec loss:[0.89709053 2.08548134 2.07434883 2.52884031 3.87223934 4.05275776], Sum15.510758112393198\n",
      "per sec loss:[ 2.27649653  3.96858621  6.15622276  9.17365548  9.14639877 13.67139975], Sum44.392759506130815\n",
      "per sec loss:[0.55837091 0.67998052 0.73877775 1.04019572 1.81494833 2.11043298], Sum6.942706204062635\n",
      "per sec loss:[ 1.00907982  2.51317071  3.75628348  6.644714    9.0048745  10.39061502], Sum33.31873753159314\n",
      "per sec loss:[ 1.87081396 13.46515426 13.49463167 13.81397772 15.90870573 16.77345998], Sum75.32674332086225\n",
      "per sec loss:[0.67707349 0.90802983 1.39992484 2.15086844 3.10241288 7.90019357], Sum16.138503045705573\n",
      "per sec loss:[0.16033898 0.31193998 0.26926067 0.56424124 1.23783286 2.02945377], Sum4.573067502125552\n",
      "per sec loss:[1.53435293 3.42744386 4.36697854 5.86418006 0.8191981  1.0459096 ], Sum17.058063086342116\n",
      "per sec loss:[0.57864948 0.78296704 0.18540115 0.09672641 0.86992671 1.04563908], Sum3.559309871899666\n",
      "error, num:  [ 1.88078184  5.50592514  5.95345845 10.93678309  0.          0.        ] [2 2 2 2 0 0]\n",
      "per sec loss:[0.94039092 2.75296257 2.97672923 5.46839154 0.         0.        ], Sum12.138474258975194\n",
      "per sec loss:[0.94294787 2.47153986 3.14388306 5.65734458 9.11356742 7.99098393], Sum29.320266733054687\n",
      "per sec loss:[1.71550685 2.43444584 3.4581178  4.72395075 6.4176018  7.89048136], Sum26.640104385444058\n",
      "per sec loss:[1.8875525  2.28296391 4.61048605 5.6990193  4.90820967 6.57761934], Sum25.96585077032148\n",
      "per sec loss:[0.17487944 0.4362321  0.01768046 0.10987523 0.41831936 0.53344141], Sum1.6904280002929317\n",
      "error, num:  [5.15477728 1.76513789 1.34789758 0.         0.         0.        ] [3 1 1 0 0 0]\n",
      "per sec loss:[1.71825909 1.76513789 1.34789758 0.         0.         0.        ], Sum4.831294563400932\n",
      "per sec loss:[3.17249908 6.27576934 9.52824054 1.19404109 1.33248217 1.5326321 ], Sum23.03566431823904\n",
      "per sec loss:[0.31824424 0.5678087  0.68311443 0.9838228  0.73984054 0.71969649], Sum4.012527198272611\n",
      "error, num:  [12.65806421 14.48047738  6.32679075  5.38438977  0.          0.        ] [11  8  3  2  0  0]\n",
      "per sec loss:[1.15073311 1.81005967 2.10893025 2.69219489 0.         0.        ], Sum7.761917917768317\n",
      "per sec loss:[ 2.10268177  4.75431125  6.94406386 11.71946964  5.20784614  6.06324127], Sum36.79161392581725\n",
      "error, num:  [1.31209146 2.17895969 0.         0.         0.         0.        ] [1 1 0 0 0 0]\n",
      "per sec loss:[1.31209146 2.17895969 0.         0.         0.         0.        ], Sum3.4910511550632766\n",
      "per sec loss:[ 3.05261814  3.64396762  5.24711377  6.70596697  9.78832427 11.69072726], Sum40.12871802988415\n",
      "per sec loss:[1.59950323 3.42250426 4.50194462 4.25363069 6.29393519 7.51982144], Sum27.591339418884676\n",
      "per sec loss:[0.20126191 0.15220966 0.38449682 0.72288029 0.57008869 0.69624918], Sum2.7271865408920255\n",
      "per sec loss:[0.78433239 1.41718418 1.6282126  2.6173112  4.21450558 4.33450895], Sum14.99605491086029\n",
      "per sec loss:[2.06717843 2.41442163 2.63654067 3.16527201 5.18382941 5.9479014 ], Sum21.41514355028648\n",
      "per sec loss:[1.01027556 1.81013844 3.31482262 5.18754582 6.11537149 7.90860307], Sum25.346756997180837\n",
      "per sec loss:[ 2.38697431 10.37996478 12.74161507 15.51243445 18.61330345 21.35890024], Sum80.99319228718528\n",
      "per sec loss:[0.46167566 0.75418123 0.77904135 1.01627427 0.77130519 0.79490333], Sum4.577381022898134\n",
      "per sec loss:[0.08194191 0.04670792 0.42203577 0.55187803 1.45861407 3.25847771], Sum5.819655412803522\n",
      "error, num:  [17.7269623  15.23396289  4.82459134  0.          0.          0.        ] [5 4 2 0 0 0]\n",
      "per sec loss:[3.54539246 3.80849072 2.41229567 0.         0.         0.        ], Sum9.766178854138104\n",
      "per sec loss:[0.62645376 2.51230957 3.03213031 6.99128013 8.47700898 9.60993243], Sum31.249115179611387\n",
      "per sec loss:[0.89309573 1.33599349 2.00551861 2.62020692 4.8980325  5.88283714], Sum17.635684377085084\n",
      "per sec loss:[1.16125538 1.72356055 4.31006132 5.49541854 7.07950021 8.51823897], Sum28.288034965843096\n",
      "per sec loss:[0.58407657 0.57602364 0.41470964 0.35127225 0.90715307 1.22402328], Sum4.057258449353739\n",
      "per sec loss:[0.79430839 2.87413922 3.77143222 4.82128108 6.4394645  8.14558442], Sum26.8462098254163\n",
      "per sec loss:[1.08072795 2.25563174 3.30924243 4.97800692 7.17703539 8.30567262], Sum27.10631704772802\n",
      "error, num:  [ 4.95424704  8.87946883 13.4962381  18.21484023 22.52830035  0.        ] [1 1 1 1 1 0]\n",
      "per sec loss:[ 4.95424704  8.87946883 13.4962381  18.21484023 22.52830035  0.        ], Sum68.07309455073516\n",
      "per sec loss:[1.28733274 1.67868298 2.4066026  2.34115824 2.34130562 1.83763486], Sum11.89271705044794\n",
      "error, num:  [1.92344898 0.         0.         0.         0.         0.        ] [2 0 0 0 0 0]\n",
      "per sec loss:[0.96172449 0.         0.         0.         0.         0.        ], Sum0.9617244877162932\n",
      "per sec loss:[2.43047741 3.37373432 4.5165821  5.05918573 6.04351031 6.50548264], Sum27.928972505916036\n",
      "error, num:  [ 4.36181178  9.23673134 24.75752282 23.83229279  8.0001107   0.        ] [6 6 4 3 1 0]\n",
      "per sec loss:[0.72696863 1.53945522 6.18938071 7.9440976  8.0001107  0.        ], Sum24.400012854463775\n",
      "per sec loss:[0.1860016  0.43224473 0.10099707 0.06563134 0.45341717 0.44583381], Sum1.6841257169580135\n",
      "error, num:  [2.49095806 5.04568465 0.         0.         0.         0.        ] [1 1 0 0 0 0]\n",
      "per sec loss:[2.49095806 5.04568465 0.         0.         0.         0.        ], Sum7.536642707482194\n",
      "per sec loss:[1.96806688 3.17614293 4.54926582 5.69798523 6.22469615 6.17844765], Sum27.79460465858935\n",
      "per sec loss:[0.89541054 1.55141567 1.26878858 1.45703244 0.70506571 0.96185697], Sum6.839569919408404\n",
      "per sec loss:[0.47993245 1.14683669 1.25758556 1.2395184  0.83684144 0.67192207], Sum5.632636612471096\n",
      "per sec loss:[1.12002702 1.64349235 2.25598105 2.92433198 3.50446929 4.83505151], Sum16.283353193635683\n",
      "error, num:  [5.07092975 7.31256896 8.7965781  5.41220914 5.24608803 0.        ] [4 3 2 1 1 0]\n",
      "per sec loss:[1.26773244 2.43752299 4.39828905 5.41220914 5.24608803 0.        ], Sum18.761841648751513\n",
      "per sec loss:[ 2.28724725  3.83125186  5.56898325  7.9399523   9.19491224 10.50658783], Sum39.32893472990339\n",
      "per sec loss:[0.6950318  3.02822146 3.20569119 3.20884759 3.58505725 4.34868876], Sum18.071538049452514\n",
      "per sec loss:[7.6173166  6.19752065 5.31117264 3.36899212 2.2084785  3.93988413], Sum28.643364641081273\n",
      "error, num:  [ 2.394313    5.64409712  8.88143059 11.35992977  0.          0.        ] [1 1 1 1 0 0]\n",
      "per sec loss:[ 2.394313    5.64409712  8.88143059 11.35992977  0.          0.        ], Sum28.279770481436397\n",
      "per sec loss:[0.18244999 0.29825222 0.16075948 0.21884434 0.4886628  0.60131928], Sum1.95028811661784\n",
      "per sec loss:[ 0.26290572  0.53956338  1.48793794  2.66328373  5.57341135 15.24960351], Sum25.77670563290326\n",
      "per sec loss:[0.32385089 0.45302139 0.33280764 0.17656932 0.87962308 1.19033053], Sum3.356202852913463\n",
      "per sec loss:[1.67201149 0.91646177 2.23796041 3.5768408  5.71711159 6.24787883], Sum20.368264885118844\n",
      "per sec loss:[0.30292151 1.43327212 7.80924309 5.38369155 6.0560172  6.96492514], Sum27.950070611380877\n",
      "per sec loss:[1.761617   3.26259113 4.4021864  6.69091947 6.90368972 0.90834979], Sum23.929353512566006\n",
      "per sec loss:[0.50873942 0.67441946 0.22872111 0.51359229 0.25251583 0.58151003], Sum2.759498133870518\n",
      "per sec loss:[1.59922899 3.37793994 3.62808521 4.39863638 4.97344682 5.5983887 ], Sum23.575726038509753\n",
      "per sec loss:[ 4.71762038  7.94714743 11.64909018 15.34267363 18.46105051 23.96953005], Sum82.08711217339933\n",
      "per sec loss:[ 2.20908717  7.24829274 10.08514486 10.3264434  10.3450987   9.7005963 ], Sum49.91466316560091\n",
      "per sec loss:[0.47691309 0.54745778 0.12239621 0.09375854 0.64211633 0.82233548], Sum2.7049774482942563\n",
      "per sec loss:[ 4.65978659  6.41955137  7.24354226  7.79660706  9.23519108 11.21984098], Sum46.57451933910696\n",
      "error, num:  [0.72533541 0.         0.         0.         0.         0.        ] [1 0 0 0 0 0]\n",
      "per sec loss:[0.72533541 0.         0.         0.         0.         0.        ], Sum0.7253354129607429\n",
      "per sec loss:[0.91766656 1.18979268 1.33704208 1.54336059 1.69608795 1.730551  ], Sum8.414500845477491\n",
      "per sec loss:[1.0773309  2.42064778 2.93479456 0.22019999 0.88120166 1.19224961], Sum8.726424504851181\n",
      "per sec loss:[ 1.137036    2.01347478  3.75591048  9.448258   11.33686359 14.19391539], Sum41.8854582432415\n",
      "per sec loss:[0.52733117 0.56283386 0.42308471 1.33514035 1.69683604 2.33746854], Sum6.882694673099863\n",
      "per sec loss:[1.1907254  1.5122577  1.7600594  2.69750714 3.11103565 3.63001171], Sum13.90159699451472\n",
      "per sec loss:[ 0.94334573  1.74310969  2.96468865  8.56023026  9.06742712 10.3528851 ], Sum33.63168655607341\n",
      "per sec loss:[0.56767556 0.89456393 0.63733743 0.87368465 0.5683218  0.33941216], Sum3.8809955435981625\n",
      "per sec loss:[1.85653946 3.12420396 3.86726381 4.79358622 5.07944492 5.78913745], Sum24.510175828051306\n",
      "error, num:  [0. 0. 0. 0. 0. 0.] [0 0 0 0 0 0]\n",
      "per sec loss:[0. 0. 0. 0. 0. 0.], Sum0.0\n",
      "per sec loss:[0.26280806 0.43217863 0.10494251 0.24738819 0.20061484 0.27331123], Sum1.5212434603270133\n",
      "per sec loss:[ 5.05467521  6.5499961   7.38620612  9.02825088 10.87775919  1.4274875 ], Sum40.32437500062696\n",
      "per sec loss:[1.99635884 3.77465071 5.38958308 6.55805093 7.40934735 9.01659375], Sum34.14458465181299\n",
      "per sec loss:[0.96576935 1.22673591 1.6154441  3.34576242 3.84803491 4.45174025], Sum15.453486943997945\n",
      "error, num:  [10.19472035 13.45516745  1.43501946  1.6445806   0.          0.        ] [5 3 1 1 0 0]\n",
      "per sec loss:[2.03894407 4.48505582 1.43501946 1.6445806  0.         0.        ], Sum9.603599944432982\n",
      "per sec loss:[3.67875647 3.98110484 2.59368096 1.70665462 4.52761231 5.03777796], Sum21.52558715252566\n",
      "per sec loss:[0.17301228 0.40014651 0.3370781  0.24582713 0.95069916 1.1215678 ], Sum3.228330995563934\n",
      "per sec loss:[1.36931182 5.15339763 4.00964865 3.99860827 4.04334885 3.92820426], Sum22.50251947404093\n",
      "error, num:  [1.46833573 2.23916144 4.49534345 0.         0.         0.        ] [1 1 1 0 0 0]\n",
      "per sec loss:[1.46833573 2.23916144 4.49534345 0.         0.         0.        ], Sum8.202840619226073\n",
      "per sec loss:[0.74605509 3.00161196 2.79721706 2.2683524  2.98133659 3.19390318], Sum14.988476290157681\n",
      "per sec loss:[0.13904624 0.28805746 0.15089757 0.18373351 0.4664927  0.47287086], Sum1.7010983352965399\n",
      "per sec loss:[0.75433844 1.21636605 3.00637132 3.35288687 3.27602245 3.93969467], Sum15.54567980336756\n",
      "per sec loss:[0.9048444  0.70154241 1.26298701 3.24252829 3.07525796 3.3208881 ], Sum12.508048165087764\n",
      "error, num:  [2.54302808 0.         0.         0.         0.         0.        ] [1 0 0 0 0 0]\n",
      "per sec loss:[2.54302808 0.         0.         0.         0.         0.        ], Sum2.543028083423402\n",
      "per sec loss:[1.48076128 2.41487503 3.32706654 4.59932806 4.96054887 6.37851973], Sum23.16109950838019\n",
      "per sec loss:[1.94192218 4.44559122 5.06060405 6.65348426 7.56171867 7.91956898], Sum33.582889356116034\n",
      "per sec loss:[1.4699093  3.56162671 5.85122032 4.14838844 5.49713821 5.68634385], Sum26.214626836263815\n",
      "per sec loss:[1.03515574 2.13735023 3.8864251  6.18866657 6.14629413 7.55934848], Sum26.953240257950885\n",
      "per sec loss:[2.08919415 3.28141558 4.67996355 6.4275601  8.15473298 8.13305928], Sum32.76592563791335\n",
      "per sec loss:[0.37511447 0.54675406 0.02595918 0.21915457 0.38671236 0.52928688], Sum2.0829815153112463\n",
      "per sec loss:[0.54772425 1.69120431 6.89451161 7.26927969 8.30052336 9.28603869], Sum33.98928190772952\n",
      "per sec loss:[2.96721195 3.78052331 6.66156275 8.82897609 8.94050183 8.50518506], Sum39.68396098773168\n",
      "per sec loss:[4.78842465 5.22760427 5.66045994 6.35733939 7.55979887 8.84839741], Sum38.44202454045449\n",
      "per sec loss:[ 1.08130092  1.77055745  4.31036018 14.19249294 14.57968283 15.52914875], Sum51.46354307156575\n",
      "per sec loss:[0.89131029 1.06698276 1.3676141  1.63492755 1.82585054 2.08707519], Sum8.873760431664644\n",
      "per sec loss:[1.82202026 2.44088916 2.40846405 2.54666504 2.67523648 3.10541139], Sum14.99868638701804\n",
      "per sec loss:[3.1120281  4.24669812 5.69079589 7.24972498 7.73625622 8.5212868 ], Sum36.5567900927469\n",
      "per sec loss:[1.44202014 2.60806948 3.81184982 5.68548675 8.54882802 7.03757661], Sum29.133830820073868\n",
      "per sec loss:[0.63181553 0.8633806  1.23162664 1.54813101 1.96947481 2.23159174], Sum8.47602032278096\n",
      "per sec loss:[0.57194419 0.80767784 0.91347209 1.12222584 1.68985091 1.8899265 ], Sum6.9950973737314435\n",
      "per sec loss:[0.3192035  0.4696671  0.29052358 0.22666314 1.01131293 1.25661932], Sum3.573989584600235\n",
      "per sec loss:[2.26433042 2.30431733 2.15273533 2.43381851 2.40297454 2.92838179], Sum14.486557913224152\n",
      "per sec loss:[ 1.52034712  6.91937371  9.15411388 10.168655   11.484666   13.41683066], Sum52.66398637230344\n",
      "per sec loss:[1.76200275 2.64910595 3.66410745 0.18625181 0.78022851 1.0231688 ], Sum10.064865256906836\n",
      "per sec loss:[0.66630378 3.52346991 4.02782843 4.71342354 6.37515721 7.47419246], Sum26.780375330462842\n",
      "per sec loss:[1.13736893 1.17531035 1.37599761 1.69853906 2.04596793 4.34675542], Sum11.779939301545767\n",
      "per sec loss:[0.90635278 1.31646123 1.95332436 2.29115328 2.81488309 3.79515107], Sum13.077325807969833\n",
      "error, num:  [8.87519251 5.14690049 5.17836995 0.         0.         0.        ] [2 1 1 0 0 0]\n",
      "per sec loss:[4.43759625 5.14690049 5.17836995 0.         0.         0.        ], Sum14.762866693487656\n",
      "per sec loss:[0.17487496 0.3265449  0.42723801 0.71113119 0.35478179 0.39080877], Sum2.385379615585663\n",
      "per sec loss:[0.65857157 0.91078672 0.13122721 0.02953142 0.61324594 0.74299012], Sum3.086352988096104\n",
      "per sec loss:[ 3.43489807  5.4212901   7.61153225  9.36567178 12.49140456 14.35586377], Sum52.680660539148775\n",
      "per sec loss:[1.17426819 1.96571928 2.76510748 3.10645674 3.12643284 3.04536176], Sum15.183346293798122\n",
      "error, num:  [1.52027989 0.         0.         0.         0.         0.        ] [2 0 0 0 0 0]\n",
      "per sec loss:[0.76013994 0.         0.         0.         0.         0.        ], Sum0.7601399447437921\n",
      "per sec loss:[1.37307606 1.69208572 2.2012928  2.70361653 3.3777751  4.32404636], Sum15.671892574345797\n",
      "per sec loss:[1.08287634 1.7326899  2.92237671 3.47831715 4.75592909 6.18294461], Sum20.155133796117923\n",
      "per sec loss:[0.82837924 2.48356545 6.12822923 4.36028475 5.21383869 6.01884884], Sum25.033146198078946\n",
      "error, num:  [0. 0. 0. 0. 0. 0.] [0 0 0 0 0 0]\n",
      "per sec loss:[0. 0. 0. 0. 0. 0.], Sum0.0\n",
      "error, num:  [1.0571796  1.86343903 0.         0.         0.         0.        ] [1 1 0 0 0 0]\n",
      "per sec loss:[1.0571796  1.86343903 0.         0.         0.         0.        ], Sum2.9206186357693635\n",
      "per sec loss:[0.68810204 0.53542237 0.60350264 0.86134829 1.15372528 2.05696032], Sum5.8990609494918225\n",
      "per sec loss:[0.92317521 1.64242177 2.62471733 5.13286289 6.60450567 5.13451728], Sum22.06220015867162\n",
      "per sec loss:[1.00379949 1.13995565 3.09482842 3.54781866 4.07283835 4.39198538], Sum17.251225962278536\n",
      "error, num:  [0.53241652 2.82941492 0.         0.         0.         0.        ] [1 1 0 0 0 0]\n",
      "per sec loss:[0.53241652 2.82941492 0.         0.         0.         0.        ], Sum3.36183143408698\n",
      "per sec loss:[2.77099227 3.41664648 4.88066897 5.74450505 6.80359782 8.72674908], Sum32.34315966929187\n",
      "per sec loss:[1.05638361 1.15235963 1.14707128 1.73980659 2.03454384 2.22082519], Sum9.35099013475055\n",
      "per sec loss:[ 0.65083333  2.07058225  3.94358393  4.27787244  8.99645873 10.36846316], Sum30.307793851977863\n",
      "per sec loss:[0.2976293  0.43638499 0.23548458 0.19690098 0.28527896 0.87960473], Sum2.3312835305035504\n",
      "per sec loss:[0.88814909 0.99364858 1.26079065 1.70880681 2.6969489  5.37063475], Sum12.918978788408479\n",
      "per sec loss:[ 1.6996462   2.50251573  5.52161711  7.59519753 10.75779687 12.07955721], Sum40.15633065788933\n",
      "per sec loss:[0.40770468 0.63005888 0.24714156 0.37150964 0.36970109 0.81426416], Sum2.8403800060229427\n",
      "per sec loss:[0.62587395 1.56245135 1.94820736 1.99118086 2.39108543 2.94769396], Sum11.46649292186218\n",
      "per sec loss:[0.29065041 0.57916898 0.5697577  0.86827616 0.54246114 0.637259  ], Sum3.487573390429585\n",
      "per sec loss:[1.35155342 5.58139772 5.25587823 5.79156909 5.21738279 6.00407541], Sum29.201856654178346\n",
      "error, num:  [2.99932615 4.61077167 0.         0.         0.         0.        ] [3 2 0 0 0 0]\n",
      "per sec loss:[0.99977538 2.30538583 0.         0.         0.         0.        ], Sum3.3051612182209515\n",
      "per sec loss:[ 2.87885372  4.51892127  6.15518092  7.9947415  10.840474   13.72773974], Sum46.11591115665484\n",
      "per sec loss:[ 1.31271179  2.32529113  4.31774545  6.93815878 12.66451524 15.68136797], Sum43.23979036738095\n",
      "error, num:  [8.38782988 3.36124182 5.13754802 0.         0.         0.        ] [8 3 2 0 0 0]\n",
      "per sec loss:[1.04847874 1.12041394 2.56877401 0.         0.         0.        ], Sum4.737666683850239\n",
      "error, num:  [0. 0. 0. 0. 0. 0.] [0 0 0 0 0 0]\n",
      "per sec loss:[0. 0. 0. 0. 0. 0.], Sum0.0\n",
      "error, num:  [2.29233419 0.         0.         0.         0.         0.        ] [1 0 0 0 0 0]\n",
      "per sec loss:[2.29233419 0.         0.         0.         0.         0.        ], Sum2.292334186204803\n",
      "per sec loss:[ 1.04658218  3.37227865  4.52006308  7.36361377 15.35741457  5.7316136 ], Sum37.3915658396463\n",
      "per sec loss:[1.14801807 1.62990235 2.3225957  2.73494472 3.34318344 2.29938724], Sum13.478031526064925\n",
      "per sec loss:[1.29345686 2.01649671 2.62192475 3.83198546 8.53169502 9.3367853 ], Sum27.6323441087181\n",
      "error, num:  [0.309705   7.49218677 0.         0.         0.         0.        ] [1 1 0 0 0 0]\n",
      "per sec loss:[0.309705   7.49218677 0.         0.         0.         0.        ], Sum7.801891775096656\n",
      "per sec loss:[ 1.45556851  2.73844416  4.88699292 12.40147234  6.86564464  7.66602186], Sum36.014144437577514\n",
      "per sec loss:[0.19748108 0.47107111 0.12936084 0.20553912 0.4764967  0.54537964], Sum2.0253285002112813\n",
      "per sec loss:[0.963686   1.83865204 2.80973192 0.76235094 1.03025517 0.97634831], Sum8.38102438256993\n",
      "per sec loss:[1.83687601 2.42017821 2.69306025 3.21956156 2.55217158 3.18236204], Sum15.904209645288875\n",
      "per sec loss:[1.1563915  0.64557448 4.59422371 4.31052701 4.94786853 4.76103467], Sum20.415619892727037\n",
      "per sec loss:[0.99757974 1.71545226 2.45372475 3.54928392 5.05919663 7.17948955], Sum20.954726856487518\n",
      "per sec loss:[1.74056834 3.32667752 2.62276661 2.65372241 1.12674426 1.5825306 ], Sum13.05300973009241\n",
      "per sec loss:[ 1.42943257  3.45760958  4.31798847  3.18641971  9.77206561 10.02870697], Sum32.19222291742422\n",
      "per sec loss:[1.42585383 1.45738226 2.13840986 2.62802363 1.79930494 2.75104999], Sum12.200024513348124\n",
      "error, num:  [ 4.21484057 10.493108    5.45885284  6.06142052  0.          0.        ] [3 2 1 1 0 0]\n",
      "per sec loss:[1.40494686 5.246554   5.45885284 6.06142052 0.         0.        ], Sum18.171774224804512\n",
      "per sec loss:[1.1320294  1.9940786  2.17796755 1.90526888 2.1849393  2.41422801], Sum11.808511739646207\n",
      "per sec loss:[0.3466509  0.5989262  0.64696231 0.92442255 0.91421837 0.88361427], Sum4.314794613067825\n",
      "per sec loss:[1.44105205 1.90387252 4.42553223 5.28964038 2.84740197 2.04103488], Sum17.948534038534632\n",
      "error, num:  [0. 0. 0. 0. 0. 0.] [0 0 0 0 0 0]\n",
      "per sec loss:[0. 0. 0. 0. 0. 0.], Sum0.0\n",
      "per sec loss:[1.18202624 6.60994663 6.88017799 7.76930001 8.63269726 9.87865286], Sum40.952800993734954\n",
      "per sec loss:[0.57668117 0.88899005 0.8353892  0.11050491 0.59852422 0.84737985], Sum3.8574693954913624\n",
      "per sec loss:[1.48205953 1.7240838  3.0810672  3.52399796 3.87358993 4.36853084], Sum18.05332926166995\n",
      "per sec loss:[ 2.68611444  4.00552141 10.08114108 12.83085588 14.6344979  16.31068954], Sum60.54882025456587\n",
      "per sec loss:[1.5803778  2.57514595 3.49312025 4.75085326 4.89361145 4.85372355], Sum22.146832268382045\n",
      "per sec loss:[0.95277721 1.25259997 1.94011814 3.95250476 5.14502433 6.31219237], Sum19.555216775003302\n",
      "per sec loss:[0.40907634 3.37405798 3.33265218 3.79659663 4.26876693 4.81038944], Sum19.99153950281946\n",
      "per sec loss:[1.74672759 4.36432224 6.3088948  8.18311951 0.38030676 0.55802188], Sum21.541392783490423\n",
      "per sec loss:[2.4746178  4.87665961 3.84977697 3.24340508 1.62976396 1.87793502], Sum17.9521584449058\n",
      "error, num:  [ 3.85163973  8.38877705 12.18829002  0.          0.          0.        ] [2 2 2 0 0 0]\n",
      "per sec loss:[1.92581987 4.19438853 6.09414501 0.         0.         0.        ], Sum12.214353400286624\n",
      "per sec loss:[0.94157397 4.05701762 5.04090875 2.02136447 2.23806331 3.97604356], Sum18.27497168075807\n",
      "per sec loss:[1.85331627 2.49712828 3.79650762 5.3221707  6.85886683 8.89552126], Sum29.223510959397046\n",
      "per sec loss:[0.15304834 0.29063259 0.16150133 0.16773808 0.52515218 0.6935959 ], Sum1.9916684273138145\n",
      "error, num:  [7.84090306 0.         0.         0.         0.         0.        ] [1 0 0 0 0 0]\n",
      "per sec loss:[7.84090306 0.         0.         0.         0.         0.        ], Sum7.840903058184134\n",
      "error, num:  [6.04196822 6.31680986 5.7083728  6.105701   0.96023628 0.        ] [1 1 1 1 1 0]\n",
      "per sec loss:[6.04196822 6.31680986 5.7083728  6.105701   0.96023628 0.        ], Sum25.133088153953373\n",
      "per sec loss:[1.14598683 0.88632511 1.70654357 2.67641444 3.24630921 4.88913032], Sum14.550709472834836\n",
      "per sec loss:[0.57979649 1.10042901 0.65075712 3.47796114 3.02631225 3.5461421 ], Sum12.38139812072004\n",
      "per sec loss:[3.49152649 4.10147582 4.64943078 6.02861286 7.36895525 8.35624036], Sum33.996241554922314\n",
      "per sec loss:[ 1.83012006  4.7126832   8.13411837 10.53709059 13.17332763  0.94304923], Sum39.33038908074233\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/loss': tensor(20.9081),\n",
      " 'test/loss_1': tensor(2.0629, dtype=torch.float64),\n",
      " 'test/loss_2': tensor(3.7289, dtype=torch.float64),\n",
      " 'test/loss_3': tensor(4.6622, dtype=torch.float64)}\n",
      "--------------------------------------------------------------------------------\n",
      "Loss variance:  [0.9462914120935758, 1.6226994973248055, 2.3402426633159847, 3.009011498987273, 3.6305276164984006, 3.8419754123517107]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test/loss': 20.90811538696289,\n",
       "  'test/loss_1': 2.0629425772652454,\n",
       "  'test/loss_2': 3.7289247454947643,\n",
       "  'test/loss_3': 4.66218974150357}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class GatedGCN_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.A = nn.Linear(input_dim, output_dim)\n",
    "        self.B = nn.Linear(input_dim, output_dim)\n",
    "        self.C = nn.Linear(input_dim, output_dim)\n",
    "        self.D = nn.Linear(input_dim, output_dim)\n",
    "        self.E = nn.Linear(input_dim, output_dim)\n",
    "        self.bn_node_h = nn.BatchNorm1d(output_dim)\n",
    "        self.bn_node_e = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        Bh_j = edges.src['Bh']\n",
    "        # e_ij = Ce_ij + Dhi + Ehj   N*B,256\n",
    "        e_ij = edges.data['Ce'] + edges.src['Dh'] + edges.dst['Eh']\n",
    "        edges.data['e'] = e_ij\n",
    "        return {'Bh_j' : Bh_j, 'e_ij' : e_ij}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        Ah_i = nodes.data['Ah']\n",
    "        Bh_j = nodes.mailbox['Bh_j']\n",
    "        e = nodes.mailbox['e_ij']\n",
    "        # sigma_ij = sigmoid(e_ij)\n",
    "        torch.clamp(e.sigmoid_(), min=1e-4, max=1-1e-4) \n",
    "        sigma_ij = torch.sigmoid(e)\n",
    "        # hi = Ahi + sum_j eta_ij * Bhj   \n",
    "        h = Ah_i + torch.sum(sigma_ij * Bh_j, dim=1) / torch.sum(sigma_ij, dim=1)  #shape n_nodes*256\n",
    "        return {'h' : h}\n",
    "    \n",
    "    def forward(self, g, h, e, snorm_n, snorm_e):\n",
    "        \n",
    "        h_in = h # residual connection\n",
    "        e_in = e # residual connection\n",
    "        \n",
    "        \n",
    "        g.ndata['h']  = h \n",
    "        g.ndata['Ah'] = self.A(h) \n",
    "        g.ndata['Bh'] = self.B(h) \n",
    "        g.ndata['Dh'] = self.D(h)\n",
    "        g.ndata['Eh'] = self.E(h) \n",
    "        g.edata['e']  = e \n",
    "        g.edata['Ce'] = self.C(e)\n",
    "        \n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        \n",
    "        h = g.ndata['h'] # result of graph convolution\n",
    "        e = g.edata['e'] # result of graph convolution\n",
    "\n",
    "        \n",
    "        h = h * snorm_n # normalize activation w.r.t. graph node size\n",
    "        e = e * snorm_e # normalize activation w.r.t. graph edge size\n",
    "        \n",
    "        h = self.bn_node_h(h) # batch normalization  \n",
    "        e = self.bn_node_e(e) # batch normalization  \n",
    "        \n",
    "        h = torch.relu(h) # non-linear activation\n",
    "        e = torch.relu(e) # non-linear activation\n",
    "        \n",
    "        h = h_in + h # residual connection\n",
    "        e = e_in + e # residual connection\n",
    "        \n",
    "        return h, e\n",
    "\n",
    "\n",
    "class GatedGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.embedding_e = nn.Linear(1, hidden_dim)\n",
    "        self.GatedGCN1 = GatedGCN_layer(hidden_dim, hidden_dim)\n",
    "        self.GatedGCN2 = GatedGCN_layer(hidden_dim, hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, output_dim)\n",
    "        '''\n",
    "        if dropout:\n",
    "            self.linear_dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.linear_dropout =  nn.Dropout(0.)\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn = bn\n",
    "        '''\n",
    "        \n",
    "    def forward(self, g, inputs, e, snorm_n, snorm_e):\n",
    "\n",
    "        #reshape to have shape (B*V,T*C) [c1,c2,...,c6]\n",
    "        inputs = inputs.view(inputs.shape[0],-1)\n",
    "\n",
    "        # input embedding\n",
    "        h = self.embedding_h(inputs)\n",
    "        e = self.embedding_e(e)\n",
    "        # graph convnet layers\n",
    "        h, e = self.GatedGCN1(g, h, e, snorm_n, snorm_e)\n",
    "        h, e = self.GatedGCN2(g, h, e, snorm_n, snorm_e)\n",
    "        # MLP \n",
    "        \n",
    "        y = self.linear1(h)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, collate_fn=collate_batch)\n",
    "val_dataloader=DataLoader(val_dataset, batch_size=batch_size, shuffle=False,  num_workers=8, collate_fn=collate_batch)\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=8, collate_fn=collate_batch)  \n",
    "\n",
    "#gcn_model = GCN(in_feats=18, hid_feats=256, out_feats=12).to(dev)\n",
    "#gat_model = My_GAT(input_dim=24, hidden_dim=511, output_dim=12,dropout=0.25, bn=False, bn_gat=False, feat_drop=False, attn_drop=False).to(dev)\n",
    "#gat_model.load_state_dict(torch.load('./models_checkpoints/efficient-sweep-8.ckpt'))\n",
    "#gcn_model.load_state_dict(torch.load('./models_checkpoints/gcn_256_b64_ep40_embed.pth'))\n",
    "model_type='gated'\n",
    "if model_type == 'gat':\n",
    "    model = My_GAT(input_dim=30, hidden_dim=511, heads=1, output_dim=12,dropout=0.1, bn=False, bn_gat=False, feat_drop=0, attn_drop=0, att_ew=True)\n",
    "elif model_type == 'gcn':\n",
    "    model = model = GCN(in_feats=30, hid_feats=config.hidden_dims, out_feats=12, dropout=0, gcn_drop=0, bn=False, gcn_bn=False)\n",
    "elif model_type == 'gated':\n",
    "    model = GatedGCN(input_dim=30, hidden_dim=511, output_dim=12)\n",
    "\n",
    "wandb.init(project='dbu_graph')\n",
    "wandb_logger = pl_loggers.WandbLogger(save_dir='./logs/')\n",
    "#init model\n",
    "LitGCN_sys = LitGNN(model=model, lr=1e-4, model_type='gat',wd=0.1)\n",
    "print(LitGCN_sys.model)\n",
    "LitGCN_sys = LitGCN_sys.load_from_checkpoint(checkpoint_path='./models_checkpoints/swift-sweep-9.ckpt',model=LitGCN_sys.model)\n",
    "trainer = pl.Trainer(gpus=1, profiler=True, logger= wandb_logger)\n",
    "\n",
    "trainer.test(LitGCN_sys, test_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:10<00:00, 22.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2020-11-24 16:34:06.548013| Test_RMSE: 1.650 2.477 3.415 4.310 5.249 6.020 23.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xy_s1_list= []\n",
    "xy_s2_list= []\n",
    "xy_s3_list= []\n",
    "num_s1_list= []\n",
    "num_s2_list= []\n",
    "num_s3_list= []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    overall_num_list=[] \n",
    "    overall_x2y2_list=[]\n",
    "    for batched_graph, output_masks,snorm_n, snorm_e in tqdm(test_dataloader):\n",
    "        feats = batched_graph.ndata['x'].float().to(dev)\n",
    "        feats = feats.view(feats.shape[0],-1)\n",
    "        e_w = batched_graph.edata['w'].float().to(dev)\n",
    "\n",
    "        #for GatedGCN\n",
    "        #e_w= e_w.view(e_w.shape[0],1)\n",
    "\n",
    "        labels= batched_graph.ndata['gt'][:,:,:].float().to(dev)\n",
    "        #labels = labels.view(labels.shape[0], -1)\n",
    "        pred = model(batched_graph.to(dev), feats,e_w,snorm_n,snorm_e)\n",
    "        _, overall_num, x2y2_error = compute_RMSE_batch(pred, labels, output_masks.to(dev))\n",
    "        #print(x2y2_error.shape)  #BV,T\n",
    "        overall_num_list.extend(overall_num.detach().cpu().numpy())\n",
    "        #print(overall_num.shape)  #BV,T\n",
    "        overall_x2y2_list.extend((x2y2_error**0.5).detach().cpu().numpy())  #RMSE para cada nodo en cada T\n",
    "\n",
    "overall_sum_time=np.sum(overall_x2y2_list,axis=0)  #BV,T->T RMSE medio en cada T\n",
    "overall_num_time =np.sum(overall_num_list, axis=0)\n",
    "overall_loss_time=(overall_sum_time / overall_num_time) #media del error de cada agente en cada frame\n",
    "\n",
    "print('|{}| Test_RMSE: {}'.format(datetime.now(), ' '.join(['{:.3f}'.format(x) for x in list(overall_loss_time) + [np.sum(overall_loss_time)]])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
